defaults:
  # Common setup configs
  - setup

  # The environment family and its default config, e.g., name, truncate_env_steps_at, kwargs, etc.
  # Also contains the models (actor and critic) configs. E.g., num_layers, hidden_size, activation, etc.
  # Refer to configs/env/example.yaml.
  # Can be switched like selectors e.g. python ... env=gym-atari
  - env: gym-classic-control

  # The algorithm and its default config, e.g., loss, replay buffer, etc.
  # Refer to configs/algo/example.yaml.
  # Can be switched like selectors e.g. python ... env=reinforce
  - algo: ppo-clip

  # This file.
  # Logging, optimizer, value estimator, device.
  - _self_

  # Optional custom default config for specific algorithm-environment combinations.
  - optional env_algo: ${env}-${algo}

  # Optional override for development.
  - optional override: solve

###################################################################################################

job_subdir: ${algo}/${env.lib}-${env.sublib}

debug:
  check_env_specs: False  # Checks the environment specs before and after transforms.
  benchmark_env: False    # Benchmarks the fps of the environment before and after transforms.

logging:
  log_level:
    # Frequency at which to log the training progress.
    # == -3 means every minibatch
    # == -2 means every epoch
    # == -1 means every batch
    # in [0, 100] means log every x percent of training progress.
    #   E.g., 0.1 means every 0.1% (Minimum is every batch anyway), 10 means every 10%.
    # > 100 means never log.
    0.1
  save_model_level:
    # Frequency at which to save the models of the experiment.
    # == -1 means every batch
    # in [0, 100] means save model every x percent of training progress.
    # > 100 means never save model.
    2.5

eval:
  record_video: True
  log_level:
    # Frequency at which to evaluate the model.
    # == -1 means every batch
    # in [0, 100] means evaluate every x percentage of training progress.
    # > 100 means never log.
    # An eval before the start of training and at the end of training is always performed,
    # when log_level <= 100.
    101

optim:
  # Common optimization parameters across algorithms.
  # Other specific parameters (e.g., minibatch size for PPO) for are set in the algorithm default config.
  algo: Adam
  max_grad_norm: 1.
  reset_state: False  # Reset the optimizer running means.
  kwargs:
    lr: 1e-5
    betas: [0.9, 0.999]
  anneal_linearly: False
  policy:
    algo: ${optim.algo}
    max_grad_norm: ${optim.max_grad_norm}
    reset_state: ${optim.reset_state}
    kwargs:
      lr: ${optim.kwargs.lr}
      betas: ${optim.kwargs.betas}
    anneal_linearly: ${optim.anneal_linearly}
  value:
    algo: ${optim.algo}
    max_grad_norm: ${optim.max_grad_norm}
    reset_state: ${optim.reset_state}
    kwargs:
      lr: ${optim.kwargs.lr}
      betas: ${optim.kwargs.betas}
    anneal_linearly: ${optim.anneal_linearly}

target_estimator_kwargs:
  # The parameters for the target value estimator and advantage estimator.
  # We support using an estimator for targets used to train the critic.
  # And a different estimator for targets used to train the actor.
  # GAE is preferred and allows to estimate TD(0) and TD(1) targets anyway.
  gamma: 1
  lmbda: 1
  critic:
    gamma: ${target_estimator_kwargs.gamma}
    lmbda: ${target_estimator_kwargs.lmbda}
  actor:
    gamma: ${target_estimator_kwargs.gamma}
    lmbda: ${target_estimator_kwargs.lmbda}

loss:
  value:
    kwargs:
      coef: 0.5
      loss_type: l2
      use_episodic_mask: ${collector.is_episodic}
      feature_trust_region_regularize_or_clip: "regularize"
      feature_trust_region_coef: 0.
      feature_trust_region_limit: 0.1
      feature_trust_region_type: "l2"
      feature_trust_use_preactivation: True
      feature_trust_all_layers: False

device:
  rollout:
    # Where the policy network is called during interaction with the environment.
    # No memory concerns (one obs at a time).
    ${resolve_device:rollout, cuda:0}
  collector_storage:
    # Where the collected rollout batch is stored.
    # Device should fit collector.agent_steps_per_batch frames.
    # Target estimators will be computed on this device.
    ${resolve_device:collector_storage, cuda:0}
  training:
    # Where value and policy networks are trained (forward + backward)
    # Should fit optim.minibatch_size frames.
    ${resolve_device:training, cuda:0}
