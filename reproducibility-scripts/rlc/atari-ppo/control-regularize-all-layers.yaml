project: no-representation-no-trust
name: atari-ppo-regularize-all-layers
method: grid
metric:
  goal: maximize
  name: batch/perf/avg_return_raw
parameters:
  wandb.mode:
    value: online
  wandb.tags:
    value: [rlc, experiment, control, regularize-all-layers]
  outputs_subdir:
    value: rlconf
  job_subdir:
    value: "atari-ppo/control/regularize-all-layers"
  env:
    value: gym-atari
  env.name:
    values: [ "ALE/Phoenix-v5", "ALE/NameThisGame-v5", "ALE/DoubleDunk-v5", "ALE/Qbert-v5", "ALE/Gravitar-v5", "ALE/BattleZone-v5" ]
  seed:
    values: [ 25, 7, 64, 27, 4 ]
  optim.num_epochs:
    values: [ 4, 6, 8 ]
  loss.policy.kwargs.feature_trust_region_coef:
    value: 10
  loss.policy.kwargs.feature_trust_all_layers:
    value: True
command:
  - python
  - "-m"
  - "po_dynamics.solve"
  - ${args_no_hyphens}
