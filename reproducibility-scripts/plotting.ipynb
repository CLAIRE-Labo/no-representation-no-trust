{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f318a1a2",
   "metadata": {},
   "source": [
    "# Notebook used to plot all the figures of the paper\n",
    "May contain dead or deprecated code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f61810",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d87968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn==0.13.2\n",
    "!pip install git+https://github.com/killiansheriff/LovelyPlots@3cfd78fb4d5a3d8c9f89feb7742f18340ffd2fb7\n",
    "!pip install brokenaxes==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy.stats import linregress, kendalltau, spearmanr, pearsonr\n",
    "from brokenaxes import brokenaxes\n",
    "from cycler import cycler\n",
    "import lovelyplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ae4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lovelyplots_installdir = os.path.dirname(lovelyplots.__file__)\n",
    "plt.style.use(\n",
    "    [\n",
    "        f\"file://{lovelyplots_installdir}/styles/ipynb.mplstyle\",\n",
    "        f\"file://{lovelyplots_installdir}/styles/colors/colors10.mplstyle\",\n",
    "        f\"file://{lovelyplots_installdir}/styles/utils/use_mathtext.mplstyle\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "line_cycler = cycler(color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"])\n",
    "\n",
    "marker_cycler = (\n",
    "    cycler(color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"])\n",
    "    + cycler(linestyle=[\"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\"])\n",
    "    + cycler(marker=[\"4\", \"2\", \"3\", \"1\", \"+\", \"x\", \".\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f0253",
   "metadata": {},
   "source": [
    "# Atari <a name=\"atari\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff4663",
   "metadata": {},
   "source": [
    "## Loading the runs from raw logs in a df and saving them combined .csv files <a name=\"load_atari\"></a>\n",
    "### Training\n",
    "<div style=\"background-color: #F9F9F9; border-left: 5px solid #CC0000; padding: 10px; margin: 20px 0;\">\n",
    "    <strong> Skip to loading the combined raw logs .csv if you don't have the raw logs.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd38634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data is somewhere else create a symlink to it as your $PROJECT_DIR/outputs/rlconf\n",
    "# ln -s <path-to-rlconf-containing-data> $PROJECT_DIR/outputs/rlconf\n",
    "\n",
    "root_log_dir = f\"../outputs/rlconf/solve/atari-ppo/baselines/2024-04-05_23-18-42-112955\"\n",
    "\n",
    "all_keys = set()\n",
    "log_subdirs = [\"logs/models\", \"logs/minibatch\", \"logs/eval\", \"logs/epoch\", \"logs/batch\"]\n",
    "n = len(os.listdir(root_log_dir))\n",
    "\n",
    "for subdir in log_subdirs:\n",
    "    subdir_path = os.path.join(root_log_dir, subdir)\n",
    "    if not os.path.exists(subdir_path):\n",
    "        print(f\"Subdirectory {subdir} does not exist.\")\n",
    "        continue\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        if file_path.endswith(\".tar\"):\n",
    "            loaded_data = torch.load(file_path, map_location=torch.device(\"cpu\"))\n",
    "            all_keys.update(loaded_data.keys())\n",
    "all_keys = list(all_keys)\n",
    "all_keys.sort()\n",
    "\n",
    "\n",
    "def get_nested_config_value(config, nested_key):\n",
    "    \"\"\"\n",
    "    Retrieve a value from a nested dictionary using a list of keys.\n",
    "    :param config: The configuration dictionary.\n",
    "    :param nested_key: A list of keys representing the path to the desired value.\n",
    "    :return: The value if found, None otherwise.\n",
    "    \"\"\"\n",
    "    for key in nested_key:\n",
    "        if isinstance(config, dict) and key in config:\n",
    "            config = config[key]\n",
    "        else:\n",
    "            return None\n",
    "    return config\n",
    "\n",
    "\n",
    "def config_matches_criteria(config, criteria):\n",
    "    \"\"\"\n",
    "    Check if the configuration matches the given criteria, supporting nested keys.\n",
    "    :param config: The configuration dictionary from the YAML file.\n",
    "    :param criteria: The criteria dictionary to match against, with nested keys as tuples.\n",
    "    :return: True if the config matches the criteria, False otherwise.\n",
    "    \"\"\"\n",
    "    for keys, value in criteria.items():\n",
    "        # Support for nested keys represented as tuples in criteria\n",
    "        nested_keys = keys if isinstance(keys, tuple) else (keys,)\n",
    "        config_value = get_nested_config_value(config, nested_keys)\n",
    "        if config_value != value:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def flatten_config(config, parent_key=\"\", sep=\"/\"):\n",
    "    items = []\n",
    "    for k, v in config.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_config(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                items.extend(flatten_config({f\"{new_key}{sep}{i}\": item}, \"\", sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def find_matching_configs(root_path, criteria):\n",
    "    \"\"\"\n",
    "    Find subfolders where the config matches the given criteria within each subfolder of root_path.\n",
    "    \"\"\"\n",
    "    matching_paths = []\n",
    "\n",
    "    # Iterate over each subfolder in the given root_path\n",
    "    for category in tqdm(os.listdir(root_path), total=len(os.listdir(root_path)), desc=\"Find matching path\"):\n",
    "        category_path = os.path.join(root_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue  # Skip if not a directory\n",
    "        if category == \"baselines\":\n",
    "            # Iterate over each run folder within the category\n",
    "            for run_folder in os.listdir(category_path):\n",
    "                config_path = os.path.join(category_path, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                # Check if the config_resolved.yaml file exists\n",
    "                if os.path.exists(config_path):\n",
    "                    with open(config_path, \"r\") as file:\n",
    "                        config = yaml.safe_load(file)\n",
    "\n",
    "                    # Check if the config matches the given criteria\n",
    "                    if config_matches_criteria(config, criteria):\n",
    "                        matching_paths.append(os.path.join(category_path, run_folder))\n",
    "        else:\n",
    "            for category2 in os.listdir(category_path):\n",
    "                category_path2 = os.path.join(category_path, category2)\n",
    "                for run_folder in os.listdir(category_path2):\n",
    "                    config_path = os.path.join(category_path2, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                    # Check if the config_resolved.yaml file exists\n",
    "                    if os.path.exists(config_path):\n",
    "                        with open(config_path, \"r\") as file:\n",
    "                            config = yaml.safe_load(file)\n",
    "\n",
    "                        # Check if the config matches the given criteria\n",
    "                        if config_matches_criteria(config, criteria):\n",
    "                            matching_paths.append(os.path.join(category_path2, run_folder))\n",
    "\n",
    "    return matching_paths\n",
    "\n",
    "\n",
    "def compile_data_debug(matching_paths, metric_keys):\n",
    "    data = []\n",
    "    nan_counter = 0  # Counter for NaN occurrences for the specific key\n",
    "\n",
    "    for path in tqdm(matching_paths, total=len(matching_paths), desc=\"Filling df\"):\n",
    "        # Load and flatten the config\n",
    "        config_path = os.path.join(path, \"config\", \"config_resolved.yaml\")\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        flat_config = flatten_config(config)\n",
    "        flat_config = {f\"config/{key}\": value for key, value in flat_config.items()}  # Prefix with \"config/\"\n",
    "\n",
    "        # Navigate to the logs/batch folder and read metrics\n",
    "        batch_folder = os.path.join(path, \"logs\", \"batch\")\n",
    "        if os.path.exists(batch_folder):\n",
    "            for log_file in os.listdir(batch_folder):\n",
    "                log_path = os.path.join(batch_folder, log_file)\n",
    "                if log_path.endswith(\".tar\"):\n",
    "                    loaded_data = torch.load(log_path, map_location=torch.device(\"cpu\"))\n",
    "                    # Initialize a record with the flattened config\n",
    "                    record = flat_config.copy()\n",
    "\n",
    "                    for metric_key in metric_keys:\n",
    "                        metric_value = loaded_data.get(metric_key, None)\n",
    "\n",
    "                        record[metric_key] = metric_value\n",
    "\n",
    "                    data.append(record)\n",
    "\n",
    "    # Create a DataFrame from the compiled data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "path_folder = f\"../outputs/rlconf/solve/atari-ppo/\"\n",
    "# Define criteria with nested keys as tuples\n",
    "criteria = {}\n",
    "matching_paths = find_matching_configs(path_folder, criteria)\n",
    "\n",
    "atari_df = compile_data_debug(matching_paths, all_keys)\n",
    "print(atari_df.head())\n",
    "\n",
    "atari_df.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/atari.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atari_df.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/atari.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the time of submission to RLC\n",
    "# Should expect 468 runs\n",
    "# 180 baselines = 6 maps * 3 epochs * 2 lr schedules * 5 seeds\n",
    "# 288 interventions = 4 interventions * 3 maps * 3 epochs * (2 lr schedules * 3 seeds + 1 lr schedule * 2 seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2261ec",
   "metadata": {},
   "source": [
    "### Plasticity\n",
    "<div style=\"background-color: #F9F9F9; border-left: 5px solid #CC0000; padding: 10px; margin: 20px 0;\">\n",
    "    <strong> Skip to loading the combined raw logs .csv if you don't have the raw logs.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3955b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_log_dir = f\"../outputs/rlconf/capacity/atari-ppo/all/2024-04-08_11-25-26-690228\"\n",
    "\n",
    "\n",
    "all_keys = set()\n",
    "log_subdirs = [\"logs/checkpoint\", \"logs/minibatch\", \"logs/model\", \"logs/epoch\"]\n",
    "n = len(os.listdir(root_log_dir))\n",
    "\n",
    "for subdir in log_subdirs:\n",
    "    subdir_path = os.path.join(root_log_dir, subdir)\n",
    "    if not os.path.exists(subdir_path):\n",
    "        print(f\"Subdirectory {subdir} does not exist.\")\n",
    "        continue\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        if file_path.endswith(\".tar\"):\n",
    "            loaded_data = torch.load(file_path, map_location=torch.device(\"cpu\"))\n",
    "            all_keys.update(loaded_data.keys())\n",
    "all_keys = list(all_keys)\n",
    "all_keys.sort()\n",
    "\n",
    "\n",
    "def find_matching_configs(root_path, criteria):\n",
    "    \"\"\"\n",
    "    Find subfolders where the config matches the given criteria within each subfolder of root_path.\n",
    "    \"\"\"\n",
    "    matching_paths = []\n",
    "\n",
    "    # Iterate over each subfolder in the given root_path\n",
    "    for category in tqdm(os.listdir(root_path), total=len(os.listdir(root_path)), desc=\"Find matching path\"):\n",
    "        category_path = os.path.join(root_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue  # Skip if not a directory\n",
    "        if category == \"all\":\n",
    "            # Iterate over each run folder within the category\n",
    "            for run_folder in os.listdir(category_path):\n",
    "                config_path = os.path.join(category_path, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                # Check if the config_resolved.yaml file exists\n",
    "                if os.path.exists(config_path):\n",
    "                    with open(config_path, \"r\") as file:\n",
    "                        config = yaml.safe_load(file)\n",
    "\n",
    "                    # Check if the config matches the given criteria\n",
    "                    if config_matches_criteria(config, criteria):\n",
    "                        matching_paths.append(os.path.join(category_path, run_folder))\n",
    "        else:\n",
    "            for category2 in os.listdir(category_path):\n",
    "                category_path2 = os.path.join(category_path, category2)\n",
    "                for run_folder in os.listdir(category_path2):\n",
    "                    config_path = os.path.join(category_path2, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                    # Check if the config_resolved.yaml file exists\n",
    "                    if os.path.exists(config_path):\n",
    "                        with open(config_path, \"r\") as file:\n",
    "                            config = yaml.safe_load(file)\n",
    "\n",
    "                        # Check if the config matches the given criteria\n",
    "                        if config_matches_criteria(config, criteria):\n",
    "                            matching_paths.append(os.path.join(category_path2, run_folder))\n",
    "\n",
    "    return matching_paths\n",
    "\n",
    "\n",
    "def compile_data_debug(matching_paths, metric_keys):\n",
    "    data = []\n",
    "    nan_counter = 0  # Counter for NaN occurrences for the specific key\n",
    "\n",
    "    for path in tqdm(matching_paths, total=len(matching_paths), desc=\"Filling df\"):\n",
    "        # Load and flatten the config\n",
    "        config_path = os.path.join(path, \"config\", \"config_resolved.yaml\")\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        flat_config = flatten_config(config)\n",
    "        flat_config = {f\"config/{key}\": value for key, value in flat_config.items()}  # Prefix with \"config/\"\n",
    "\n",
    "        # Navigate to the logs/batch folder and read metrics\n",
    "        batch_folder = os.path.join(path, \"logs\", \"checkpoint\")\n",
    "        if os.path.exists(batch_folder):\n",
    "            for log_file in os.listdir(batch_folder):\n",
    "                log_path = os.path.join(batch_folder, log_file)\n",
    "                if log_path.endswith(\".tar\"):\n",
    "                    loaded_data = torch.load(log_path, map_location=torch.device(\"cpu\"))\n",
    "                    # Initialize a record with the flattened config\n",
    "                    record = flat_config.copy()\n",
    "\n",
    "                    for metric_key in metric_keys:\n",
    "                        metric_value = loaded_data.get(metric_key, None)\n",
    "\n",
    "                        record[metric_key] = metric_value\n",
    "\n",
    "                    data.append(record)\n",
    "\n",
    "    # Create a DataFrame from the compiled data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "path_folder = f\"../outputs/rlconf/capacity/atari-ppo/\"\n",
    "# Define criteria with nested keys as tuples\n",
    "criteria = {}\n",
    "matching_paths = find_matching_configs(path_folder, criteria)\n",
    "\n",
    "atari_df_capacity = compile_data_debug(matching_paths, all_keys)\n",
    "print(atari_df_capacity.head())\n",
    "atari_df_capacity.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/atari-capacity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atari_df_capacity.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/atari-capacity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824efa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the time of submission to RLC\n",
    "# Should expect 468 runs\n",
    "# 180 baselines = 6 maps * 3 epochs * 2 lr schedules * 5 seeds\n",
    "# 288 interventions = 4 interventions * 3 maps * 3 epochs * (2 lr schedules * 3 seeds + 1 lr schedule * 2 seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c0579",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #F9F9F9; border-left: 5px solid #CC0000; padding: 10px; margin: 20px 0;\">\n",
    "    <strong> Load the combined raw logs .csv files here. If you did not generate them you can find the instructions to download them in $PROJECT_ROOT/outputs/README.md. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_df = pd.read_csv(\"../outputs/rlconf-plotting/combined-raw-logs/atari.csv\")\n",
    "atari_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc808010",
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_df_capacity = pd.read_csv(\"../outputs/rlconf-plotting/combined-raw-logs/atari-capacity.csv\")\n",
    "atari_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6149e6",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc511a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_criteria(df, criteria):\n",
    "    \"\"\"\n",
    "    Filter a DataFrame based on matching criteria, supporting multiple possible values for each criterion,\n",
    "    nested keys of varying depths, and substring matches for specific keys.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The DataFrame to filter.\n",
    "    - criteria (dict): A dictionary where keys are column names or tuples representing nested keys in the DataFrame,\n",
    "                       and values are the criteria that the column values must match. The value can be a single value,\n",
    "                       a list of possible values, or a substring to match within the column values.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame containing only the rows that match all the criteria.\n",
    "    \"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    for keys, value in criteria.items():\n",
    "        # Support for nested keys represented as tuples in criteria\n",
    "        if not isinstance(keys, tuple):\n",
    "            keys = (keys,)  # Convert single key to tuple for uniform processing\n",
    "        # Construct the column name based on nested keys\n",
    "        adjusted_column = \"/\".join(keys)\n",
    "\n",
    "        # Special handling for substring matching in specific columns\n",
    "        if adjusted_column in [\"config/working_dir\", \"config/solve_dir\"]:\n",
    "            # Check if value is a list or a single value\n",
    "            if isinstance(value, list):\n",
    "                # When value is a list, filter rows containing any of the strings in the list\n",
    "                filtered_df = filtered_df[\n",
    "                    filtered_df[adjusted_column].apply(\n",
    "                        lambda x: any(val in x for val in value) if pd.notna(x) else False\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                # When value is a single string, use str.contains for substring match\n",
    "                filtered_df = filtered_df[filtered_df[adjusted_column].str.contains(value, na=False)]\n",
    "        else:\n",
    "            # Filter based on whether the value is a list or a single value\n",
    "            if isinstance(value, list):\n",
    "                filtered_df = filtered_df[filtered_df[adjusted_column].isin(value)]\n",
    "            else:\n",
    "                filtered_df = filtered_df[filtered_df[adjusted_column] == value]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def create_mask_for_config(df, config_row, config_keys):\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    for key in config_keys:\n",
    "        # Check if both are NaN or if they are equal\n",
    "        both_na = pd.isna(df[key]) & pd.isna(config_row[key])\n",
    "        values_equal = df[key] == config_row[key]\n",
    "        mask &= both_na | values_equal\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b704b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot in figure 1\n",
    "def plot_shaded_metrics_side_by_side_plasticity_smooth(\n",
    "    df, df2, group_by_cols, x_col, metrics_keys, metrics_keys2, log_key=\"\", subplot_titles=None, title=\"\", name_=\"\"\n",
    "):\n",
    "    if name_.split(\"-\")[-1] == \"bis\":\n",
    "        color_cycler = cycler(color=[\"#D55E00\", \"#56B4E9\", \"#0072B2\", \"#009E73\", \"#E69F00\"])\n",
    "        plt.rc(\"axes\", prop_cycle=color_cycler)\n",
    "    else:\n",
    "        plt.rc(\"axes\", prop_cycle=line_cycler)\n",
    "    if isinstance(group_by_cols, list) and len(group_by_cols) == 1:\n",
    "        group_by = group_by_cols[0]\n",
    "    else:\n",
    "        group_by = group_by_cols\n",
    "\n",
    "    grouped = df.groupby(group_by)\n",
    "    num_metrics = len(metrics_keys) + 2\n",
    "    alpha = 0.05\n",
    "    fig, axs = plt.subplots(1, num_metrics, figsize=(40, 8), sharex=True)\n",
    "    special_keys = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_value_batch\",\n",
    "    ]\n",
    "    for metric_idx, metric_key in enumerate(metrics_keys):\n",
    "        for name, group in grouped:\n",
    "            if metric_key in special_keys:\n",
    "                # Calculate EMA for special keys: smoothing\n",
    "                ema_mean = group.groupby(x_col)[metric_key].mean().ewm(alpha=alpha, adjust=False).mean()\n",
    "                ema_min = group.groupby(x_col)[metric_key].min().ewm(alpha=alpha, adjust=False).mean()\n",
    "                ema_max = group.groupby(x_col)[metric_key].max().ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "                if metric_idx == 3:\n",
    "                    axs[metric_idx + 1].plot(\n",
    "                        ema_mean.index, ema_mean, label=name if metric_idx == 0 else \"\", linewidth=4.0\n",
    "                    )\n",
    "                    axs[metric_idx + 1].fill_between(ema_mean.index, ema_min, ema_max, alpha=0.3)\n",
    "                else:\n",
    "                    axs[metric_idx].plot(ema_mean.index, ema_mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                    axs[metric_idx].fill_between(ema_mean.index, ema_min, ema_max, alpha=0.3)\n",
    "            else:\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(x_col)[metric_key].mean()\n",
    "                min_val = group.groupby(x_col)[metric_key].min()\n",
    "                max_val = group.groupby(x_col)[metric_key].max()\n",
    "                if metric_idx == 3:\n",
    "                    axs[metric_idx + 1].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                    axs[metric_idx + 1].fill_between(\n",
    "                        mean.index, min_val, max_val, alpha=0.3\n",
    "                    )  # Use min and max for shaded area\n",
    "                else:\n",
    "                    axs[metric_idx].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                    axs[metric_idx].fill_between(\n",
    "                        mean.index, min_val, max_val, alpha=0.3\n",
    "                    )  # Use min and max for shaded area\n",
    "        if metric_idx == 3:\n",
    "            axs[metric_idx + 1].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[metric_idx + 1].set_ylabel(subplot_titles[metric_idx], fontsize=30)\n",
    "            axs[metric_idx + 1].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[metric_idx + 1].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[metric_idx + 1].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[metric_idx + 1].yaxis.get_offset_text().set_fontsize(25)\n",
    "        else:\n",
    "            axs[metric_idx].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[metric_idx].set_ylabel(subplot_titles[metric_idx], fontsize=30)\n",
    "            axs[metric_idx].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[metric_idx].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[metric_idx].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[metric_idx].yaxis.get_offset_text().set_fontsize(25)\n",
    "\n",
    "        if metric_key in log_key:\n",
    "            axs[metric_idx].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "\n",
    "        axs[metric_idx].grid(linestyle=\"dotted\")\n",
    "\n",
    "    for metric_idx2, metric_key2 in enumerate(metrics_keys2):\n",
    "        if metric_idx2 == 0:\n",
    "            grouped = df2.groupby(group_by)\n",
    "            for name, group in grouped:\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(\"capacity-counters/env_steps\")[metric_key2].mean()\n",
    "                min_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].min()\n",
    "                max_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].max()\n",
    "\n",
    "                axs[3].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                axs[3].fill_between(mean.index, min_val, max_val, alpha=0.3)  # Use min and max for shaded area\n",
    "            axs[len(metrics_keys) - 1].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[len(metrics_keys) - 1].set_ylabel(subplot_titles[len(metrics_keys)], fontsize=30)\n",
    "            axs[len(metrics_keys) - 1].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[len(metrics_keys) - 1].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[len(metrics_keys) - 1].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[len(metrics_keys) - 1].yaxis.get_offset_text().set_fontsize(25)\n",
    "            if metric_key2 in log_key:\n",
    "                axs[len(metrics_keys) - 1].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "        else:\n",
    "            grouped = df2.groupby(group_by)\n",
    "            for name, group in grouped:\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(\"capacity-counters/env_steps\")[metric_key2].mean()\n",
    "                min_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].min()\n",
    "                max_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].max()\n",
    "\n",
    "                axs[len(metrics_keys) + 1].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                axs[len(metrics_keys) + 1].fill_between(\n",
    "                    mean.index, min_val, max_val, alpha=0.3\n",
    "                )  # Use min and max for shaded area\n",
    "            axs[len(metrics_keys) + 1].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[len(metrics_keys) + 1].set_ylabel(subplot_titles[len(metrics_keys) + 1], fontsize=30)\n",
    "            axs[len(metrics_keys) + 1].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[len(metrics_keys) + 1].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[len(metrics_keys) + 1].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[len(metrics_keys) + 1].yaxis.get_offset_text().set_fontsize(25)\n",
    "            if metric_key2 in log_key:\n",
    "                axs[len(metrics_keys) + 1].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "        axs[len(metrics_keys) + metric_idx2].grid(linestyle=\"dotted\")\n",
    "    # fig.suptitle(title, fontsize=30)\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    lab = []\n",
    "    # Handle labels for fig 1 and additional figures\n",
    "    for label in labels:\n",
    "        if label.startswith(\"(\") and label.endswith(\")\"):\n",
    "            axs[0].text(\n",
    "                0.02,\n",
    "                0.90,\n",
    "                f\"{title}, \\n{df['config/optim/num_epochs'].values[0]} epochs\",\n",
    "                fontsize=20,\n",
    "                fontweight=\"bold\",\n",
    "                transform=axs[0].transAxes,\n",
    "            )\n",
    "            label = label[1:-1]\n",
    "            parts = label.split(\",\")\n",
    "            share = parts[0]\n",
    "            trust = float(parts[1])\n",
    "            reset = parts[2]\n",
    "            if share == \"True\":\n",
    "                lab.append(f\"Share actor and critic features\")\n",
    "            else:\n",
    "                if reset == \" True\":\n",
    "                    lab.append(\"Reset Adam\")\n",
    "                else:\n",
    "                    if trust == 1:\n",
    "                        lab.append(f\"Regularize last preactivation\")\n",
    "                    elif trust == 10:\n",
    "                        lab.append(f\"Regularize all preactivations\")\n",
    "                    else:\n",
    "                        lab.append(\"No intervention\")\n",
    "        else:\n",
    "            lab.append(label + \" epochs\")\n",
    "            axs[0].text(0.02, 0.95, f\"{title}\", fontsize=20, fontweight=\"bold\", transform=axs[0].transAxes)\n",
    "    legend_properties = {\"weight\": \"bold\"}\n",
    "\n",
    "    if len(lab) == 4:\n",
    "        box_to_anchor = (0.25, 1.05)\n",
    "\n",
    "    elif len(lab) == 3:\n",
    "        box_to_anchor = (0.4, 1.05)\n",
    "    else:\n",
    "        box_to_anchor = (0.15, 1.05)\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        lab,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=box_to_anchor,\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=30,\n",
    "        ncol=len(lab),\n",
    "        frameon=False,\n",
    "        handlelength=1,\n",
    "        handletextpad=0.5,\n",
    "        columnspacing=1,\n",
    "    )\n",
    "    plt.subplots_adjust(right=1, top=0.90)  # Adjust subplot params to make room for the legend\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/Figure-1-{name_}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to plot in figure 2\n",
    "def plot_shaded_metrics_side_by_side_smooth(\n",
    "    df, group_by_cols, x_col, metrics_keys, log_key=\"\", subplot_titles=None, title=\"\", name_=\"\"\n",
    "):\n",
    "    plt.rc(\"axes\", prop_cycle=line_cycler)\n",
    "    if isinstance(group_by_cols, list) and len(group_by_cols) == 1:\n",
    "        group_by = group_by_cols[0]\n",
    "    else:\n",
    "        group_by = group_by_cols\n",
    "\n",
    "    grouped = df.groupby(group_by)\n",
    "    num_metrics = len(metrics_keys)\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_metrics, figsize=(30, 10), sharex=True)\n",
    "    alpha = 0.05\n",
    "    for metric_idx, metric_key in enumerate(metrics_keys):\n",
    "        for name, group in grouped:\n",
    "            # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "\n",
    "            ema_mean = group.groupby(x_col)[metric_key].mean().ewm(alpha=alpha, adjust=False).mean()\n",
    "            ema_min = group.groupby(x_col)[metric_key].min().ewm(alpha=alpha, adjust=False).mean()\n",
    "            ema_max = group.groupby(x_col)[metric_key].max().ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "            axs[metric_idx].plot(ema_mean.index, ema_mean, label=name if metric_idx == 0 else \"\", linewidth=3.0)\n",
    "            axs[metric_idx].fill_between(ema_mean.index, ema_min, ema_max, alpha=0.3)\n",
    "\n",
    "        axs[metric_idx].set_xlabel(\"Environment steps\", fontsize=35)\n",
    "        axs[metric_idx].set_ylabel(subplot_titles[metric_idx], fontsize=35)\n",
    "        axs[metric_idx].tick_params(axis=\"x\", labelsize=35)\n",
    "        axs[metric_idx].tick_params(axis=\"y\", labelsize=35)\n",
    "        axs[metric_idx].xaxis.get_offset_text().set_fontsize(35)\n",
    "        axs[metric_idx].yaxis.get_offset_text().set_fontsize(35)\n",
    "\n",
    "        if metric_key == log_key:\n",
    "            axs[metric_idx].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "\n",
    "        axs[metric_idx].grid(linestyle=\"dotted\")\n",
    "    # Assuming the first subplot's labels represent all potential labels across subplots\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    labels = [label + \" epochs\" for label in labels]\n",
    "    legend_properties = {\"weight\": \"bold\"}\n",
    "    axs[0].text(0.02, 0.95, f\"{title}\", fontsize=20, fontweight=\"bold\", transform=axs[0].transAxes)\n",
    "    # fig.suptitle(title, fontsize=40)\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.4, 1.02),\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=35,\n",
    "        ncol=3,\n",
    "        frameon=False,\n",
    "        handlelength=1,\n",
    "        handletextpad=0.5,\n",
    "        columnspacing=1,\n",
    "    )\n",
    "    plt.subplots_adjust(right=1)  # Adjust subplot params to make room for the legend\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/Figure-2-{name_}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e2a6f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47585fd7",
   "metadata": {},
   "source": [
    "## Figure 1 <a name=\"load_atari\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure1_atari(name):\n",
    "    group_by_cols = [\"config/optim/num_epochs\"]\n",
    "\n",
    "    metrics_keys = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_value_batch\",\n",
    "    ]\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"optim\", \"reset_state\"): [False],\n",
    "        (\"config\", \"models\", \"share_features\"): [False],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0.0],\n",
    "        (\"config\", \"working_dir\"): \"baselines\",\n",
    "    }\n",
    "    fig1_df = filter_df_by_criteria(atari_df, criteria)\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"optim\", \"reset_state\"): [False],\n",
    "        (\"config\", \"models\", \"share_features\"): [False],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0.0],\n",
    "        (\"config\", \"solve_dir\"): \"baselines\",\n",
    "    }\n",
    "    fig1_df_plasticity = filter_df_by_criteria(atari_df_capacity, criteria)\n",
    "    f_name = f\"1-{name}\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    plot_shaded_metrics_side_by_side_plasticity_smooth(\n",
    "        fig1_df,\n",
    "        fig1_df_plasticity,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys,\n",
    "        [\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/value\",\n",
    "        ],\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        [\n",
    "            \"Episode return\",\n",
    "            \"Feature rank policy (PCA)\",\n",
    "            \"Norm preactivation policy\",\n",
    "            \"Feature rank critic (PCA)\",\n",
    "            \"Plasticity loss policy\",\n",
    "            \"Plasticity loss critic\",\n",
    "        ],\n",
    "        title=name,\n",
    "        name_=f_name,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36835be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_atari(\"ALE/Phoenix-v5\")\n",
    "figure1_atari(\"ALE/Qbert-v5\")\n",
    "figure1_atari(\"ALE/NameThisGame-v5\")\n",
    "figure1_atari(\"ALE/Gravitar-v5\")\n",
    "figure1_atari(\"ALE/DoubleDunk-v5\")\n",
    "figure1_atari(\"ALE/BattleZone-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e57c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure1_atari_bis(name, num_epochs):\n",
    "    group_by_cols = [\n",
    "        \"config/models/share_features\",\n",
    "        \"config/loss/policy/kwargs/feature_trust_region_coef\",\n",
    "        \"config/optim/reset_state\",\n",
    "    ]\n",
    "\n",
    "    metrics_keys = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_value_batch\",\n",
    "    ]\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [num_epochs],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"algo\"): [\"ppo-clip\"],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0, 1, 10],\n",
    "        (\"config\", \"working_dir\"): [\n",
    "            \"baseline\",\n",
    "            \"experiment\",\n",
    "            \"optimizer\",\n",
    "            \"regularize\",\n",
    "            \"regularize-all-layers\",\n",
    "            \"shared-trunk\",\n",
    "        ],\n",
    "    }\n",
    "    fig1_df = filter_df_by_criteria(atari_df, criteria)\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [num_epochs],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"algo\"): [\"ppo-clip\"],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0, 1, 10],\n",
    "        (\"config\", \"solve_dir\"): [\n",
    "            \"baseline\",\n",
    "            \"experiment\",\n",
    "            \"optimizer\",\n",
    "            \"regularize\",\n",
    "            \"regularize-all-layers\",\n",
    "            \"shared-trunk\",\n",
    "        ],\n",
    "    }\n",
    "    fig1_df_plasticity = filter_df_by_criteria(atari_df_capacity, criteria)\n",
    "    f_name = f\"1-{name}-{num_epochs}-epochs-bis\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    plot_shaded_metrics_side_by_side_plasticity_smooth(\n",
    "        fig1_df,\n",
    "        fig1_df_plasticity,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys,\n",
    "        [\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/value\",\n",
    "        ],\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        [\n",
    "            \"Episode return\",\n",
    "            \"Feature rank policy (PCA)\",\n",
    "            \"Norm preactivation policy\",\n",
    "            \"Feature rank critic (PCA)\",\n",
    "            \"Plasticity loss policy\",\n",
    "            \"Plasticity loss critic\",\n",
    "        ],\n",
    "        title=name,\n",
    "        name_=f_name,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_atari_bis(\"ALE/Phoenix-v5\", 4)\n",
    "figure1_atari_bis(\"ALE/Phoenix-v5\", 6)\n",
    "figure1_atari_bis(\"ALE/Phoenix-v5\", 8)\n",
    "\n",
    "figure1_atari_bis(\"ALE/Gravitar-v5\", 4)\n",
    "figure1_atari_bis(\"ALE/Gravitar-v5\", 6)\n",
    "figure1_atari_bis(\"ALE/Gravitar-v5\", 8)\n",
    "\n",
    "figure1_atari_bis(\"ALE/NameThisGame-v5\", 4)\n",
    "figure1_atari_bis(\"ALE/NameThisGame-v5\", 6)\n",
    "figure1_atari_bis(\"ALE/NameThisGame-v5\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b5c18",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d9045c",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f69504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure2_atari(name):\n",
    "    metrics_keys2 = [\n",
    "        \"batch/first_epoch/first_minibatch/loss/entropy\",\n",
    "        \"batch/start/action_diversity/policy_variance\",\n",
    "        \"batch/start/dead_neurons/features_policy_batch\",\n",
    "    ]\n",
    "    y_axis = [\"Entropy\", \"Policy variance\", \"Dead neurons policy\"]\n",
    "    f_name = f\"2-{name}\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"optim\", \"reset_state\"): [False],\n",
    "        (\"config\", \"models\", \"share_features\"): [False],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0.0],\n",
    "        (\"config\", \"working_dir\"): \"baselines\",\n",
    "    }\n",
    "    fig2_df = filter_df_by_criteria(atari_df, criteria)\n",
    "    group_by_cols = [\"config/optim/num_epochs\"]\n",
    "    plot_shaded_metrics_side_by_side_smooth(\n",
    "        fig2_df,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys2,\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        y_axis,\n",
    "        title=name,\n",
    "        name_=f_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cba59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure2_atari(\"ALE/Phoenix-v5\")\n",
    "figure2_atari(\"ALE/NameThisGame-v5\")\n",
    "figure2_atari(\"ALE/Gravitar-v5\")\n",
    "figure2_atari(\"ALE/BattleZone-v5\")\n",
    "figure2_atari(\"ALE/DoubleDunk-v5\")\n",
    "figure2_atari(\"ALE/Qbert-v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ae6f7",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491c4c0",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shaded_metrics_side_by_side_plasticity_fig3(\n",
    "    df, df2, group_by_cols, x_col, metrics_keys, metrics_keys2, log_key=\"\", subplot_titles=None, title=\"\", name_=\"\"\n",
    "):\n",
    "    plt.rc(\"axes\", prop_cycle=line_cycler)\n",
    "    if isinstance(group_by_cols, list) and len(group_by_cols) == 1:\n",
    "        group_by = group_by_cols[0]\n",
    "    else:\n",
    "        group_by = group_by_cols\n",
    "\n",
    "    grouped = df.groupby(group_by)\n",
    "    num_metrics = len(metrics_keys) + 1\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_metrics, figsize=(40, 8), sharex=True)\n",
    "    alpha = 0.05\n",
    "    positions = {\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\": 0,\n",
    "        \"batch/perf/avg_return_raw\": 2,\n",
    "        \"batch/diff/avg_prob_ratio_below_epsilon\": 3,\n",
    "        \"batch/end/action_diversity/policy_variance\": 4,\n",
    "        \"batch/last_epoch/last_minibatch/loss/loss_policy\": 5,\n",
    "        \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\": 1,\n",
    "    }\n",
    "    legend_info = {}  # Dictionary to track unique legends\n",
    "    for metric_idx, metric_key in enumerate(metrics_keys):\n",
    "        for name, group in grouped:\n",
    "            if metric_key in [\n",
    "                \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "                \"batch/perf/avg_return_raw\",\n",
    "                \"batch/diff/avg_prob_ratio_below_epsilon\",\n",
    "                \"batch/end/action_diversity/policy_variance\",\n",
    "                \"batch/last_epoch/last_minibatch/loss/loss_policy\",\n",
    "            ]:\n",
    "                env_name = group[\"config/env/name\"].iloc[0]  # Assuming uniform configuration within the group\n",
    "                num_epochs = group[\"config/optim/num_epochs\"].iloc[0]\n",
    "                activation = group[\"config/models/activation\"].iloc[0]\n",
    "                label = f\"{env_name}, {num_epochs} epochs\"\n",
    "\n",
    "                group = group.dropna(subset=[metric_key])\n",
    "                if metric_key == \"batch/last_epoch/last_minibatch/loss/loss_policy\":\n",
    "                    # - to have ppo-clip objective\n",
    "                    ema_mean = -group.groupby(x_col)[metric_key].mean().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_min = -group.groupby(x_col)[metric_key].min().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_max = -group.groupby(x_col)[metric_key].max().ewm(alpha=alpha, adjust=False).mean()\n",
    "                else:\n",
    "                    ema_mean = group.groupby(x_col)[metric_key].mean().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_min = group.groupby(x_col)[metric_key].min().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_max = group.groupby(x_col)[metric_key].max().ewm(alpha=alpha, adjust=False).mean()\n",
    "                (line,) = axs[positions[metric_key]].plot(ema_mean.index, ema_mean, label=label, linewidth=4.0)\n",
    "                axs[positions[metric_key]].fill_between(ema_mean.index, ema_min, ema_max, alpha=0.3)\n",
    "                axs[positions[metric_key]].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "                axs[positions[metric_key]].set_ylabel(subplot_titles[positions[metric_key]], fontsize=25)\n",
    "                axs[positions[metric_key]].tick_params(axis=\"x\", labelsize=25)\n",
    "                axs[positions[metric_key]].tick_params(axis=\"y\", labelsize=25)\n",
    "                axs[positions[metric_key]].xaxis.get_offset_text().set_fontsize(25)\n",
    "                axs[positions[metric_key]].yaxis.get_offset_text().set_fontsize(25)\n",
    "                if label not in legend_info:\n",
    "                    legend_info[label] = line\n",
    "\n",
    "        if metric_key == log_key:\n",
    "            axs[positions[metric_key]].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "\n",
    "        axs[positions[metric_key]].grid(linestyle=\"dotted\")\n",
    "\n",
    "    for metric_idx2, metric_key2 in enumerate(metrics_keys2):\n",
    "        if metric_idx2 == 0:\n",
    "            grouped = df2.groupby(group_by)\n",
    "            for name, group in grouped:\n",
    "                env_name = group[\"config/env/name\"].iloc[0]  # Assuming uniform configuration within the group\n",
    "                num_epochs = group[\"config/optim/num_epochs\"].iloc[0]\n",
    "                activation = group[\"config/models/activation\"].iloc[0]\n",
    "                label = f\"{env_name}, {num_epochs} epochs, {activation}\"\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(\"capacity-counters/env_steps\")[metric_key2].mean()\n",
    "                min_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].min()\n",
    "                max_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].max()\n",
    "\n",
    "                (line,) = axs[positions[metric_key2]].plot(mean.index, mean, label=label, linewidth=4.0)\n",
    "                axs[positions[metric_key2]].fill_between(\n",
    "                    mean.index, min_val, max_val, alpha=0.3\n",
    "                )  # Use min and max for shaded area\n",
    "            axs[positions[metric_key2]].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[positions[metric_key2]].set_ylabel(subplot_titles[positions[metric_key2]], fontsize=25)\n",
    "            axs[positions[metric_key2]].tick_params(axis=\"x\", labelsize=25)\n",
    "            axs[positions[metric_key2]].tick_params(axis=\"y\", labelsize=25)\n",
    "            axs[positions[metric_key2]].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[positions[metric_key2]].yaxis.get_offset_text().set_fontsize(25)\n",
    "\n",
    "        axs[positions[metric_key2]].grid(linestyle=\"dotted\")\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    labels = [label + \" epochs\" for label in labels]\n",
    "    color_legend = [\n",
    "        mlines.Line2D([], [], color=\"#E69F00\", linestyle=\"solid\", markersize=20, label=\"4 epochs\", linewidth=4),\n",
    "        mlines.Line2D([], [], color=\"#56B4E9\", linestyle=\"solid\", markersize=20, label=\"6 epochs\", linewidth=4),\n",
    "        mlines.Line2D([], [], color=\"#009E73\", linestyle=\"solid\", markersize=20, label=\"8 epochs\", linewidth=4),\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=list(legend_info.values()),\n",
    "        labels=list(legend_info.keys()),\n",
    "        loc=\"upper center\",  # This anchors the center of the legend at the provided coordinate\n",
    "        bbox_to_anchor=(-2.8, 1.2),  # Anchors the legend above the plot\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=30,\n",
    "        ncol=3,\n",
    "        frameon=False,\n",
    "        handlelength=1,\n",
    "        handletextpad=0.5,\n",
    "        columnspacing=1,\n",
    "    )\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/Figure-3-{name_}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure3_atari():\n",
    "    criteria = {\n",
    "        (\"config\", \"working_dir\"): [\n",
    "            \"baselines/2024-02-27_00-43-36-670838\",\n",
    "            \"baselines/2024-02-27_00-43-36-908626\",\n",
    "            \"baselines/2024-02-27_00-43-29-025809\",\n",
    "        ],\n",
    "        (\"config\", \"env\", \"name\"): [\"ALE/NameThisGame-v5\"],\n",
    "        (\"config\", \"seed\"): [7, 25],\n",
    "    }\n",
    "\n",
    "    one_game_5_df = filter_df_by_criteria(atari_df, criteria)\n",
    "\n",
    "    criteria = {\n",
    "        (\"config\", \"solve_dir\"): [\n",
    "            \"baselines/2024-02-27_00-43-36-670838\",\n",
    "            \"baselines/2024-02-27_00-43-36-908626\",\n",
    "            \"baselines/2024-02-27_00-43-29-025809\",\n",
    "        ],\n",
    "        (\"config\", \"env\", \"name\"): [\"ALE/NameThisGame-v5\"],\n",
    "    }\n",
    "\n",
    "    one_game_5_df2 = filter_df_by_criteria(atari_df_capacity, criteria)\n",
    "\n",
    "    group_by_cols = [\"config/working_dir\"]\n",
    "\n",
    "    metrics_keys2 = [\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/diff/avg_prob_ratio_below_epsilon\",\n",
    "        \"batch/end/action_diversity/policy_variance\",\n",
    "        \"batch/last_epoch/last_minibatch/loss/loss_policy\",\n",
    "    ]\n",
    "\n",
    "    plot_shaded_metrics_side_by_side_plasticity_fig3(\n",
    "        one_game_5_df,\n",
    "        one_game_5_df2,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys2,\n",
    "        [\"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\"],\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        [\n",
    "            \"Rank policy (PCA)\",\n",
    "            \"Plasticity loss policy\",\n",
    "            \"Episode return\",\n",
    "            r\"Avg of prob ratios < 1 - $\\epsilon$\",\n",
    "            \"Policy variance\",\n",
    "            \"PPO-Clip objective\",\n",
    "        ],\n",
    "        title=\"\",\n",
    "        name_=\"4\",\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7dde6e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e11903",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure3_atari()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d2bf7",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96342aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_single_metric(ax, points1, points2, colors, shapes, text1, text2, log=0):\n",
    "    # Handle log transformations\n",
    "    if log == 1:\n",
    "        points1 = np.log1p(points1)  # log1p ensures log(0 + 1) to avoid -inf\n",
    "    if log == 2:\n",
    "        points2 = np.log1p(points2)\n",
    "    if log == 3:\n",
    "        points1 = np.log1p(points1)\n",
    "        points2 = np.log1p(points2)\n",
    "\n",
    "    # Try to perform regression and plot it\n",
    "    try:\n",
    "        # Calculate linear regression and correlation metrics if data length permits\n",
    "        if len(points1) > 1 and len(points2) > 1:\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(points1, points2)\n",
    "            tau, tau_p_value = kendalltau(points1, points2)\n",
    "            rho, rho_p_value = spearmanr(points1, points2)\n",
    "\n",
    "            # Plot the regression line if regression was successful\n",
    "            sns.regplot(\n",
    "                x=points1,\n",
    "                y=points2,\n",
    "                ci=95,\n",
    "                ax=ax,\n",
    "                scatter=False,\n",
    "                line_kws={\"color\": \"#E69F00\", \"label\": f\"y={slope:.2f}x+{intercept:.2f}\"},\n",
    "            )\n",
    "            ax.set_title(f\"Kendall: {tau:.2f}, Spearman: {rho:.2f}\", fontsize=20)\n",
    "    except Exception as e:\n",
    "        # If regression fails, only plot the points\n",
    "        print(f\"Regression or correlation failed: {str(e)}\")\n",
    "\n",
    "    # Create a scatter plot with customized markers in all cases\n",
    "    for x, y, color, shape in zip(points1, points2, colors, shapes):\n",
    "        ax.scatter(x, y, color=color, marker=shape)\n",
    "\n",
    "    # Customize the subplot\n",
    "    ax.set_xlabel(text1, fontsize=20)\n",
    "    ax.set_ylabel(text2, fontsize=20)\n",
    "    if text1 == \"Feature preactivation norm\":\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.legend(loc=\"upper left\", labels=[])\n",
    "\n",
    "\n",
    "def determine_color_shape2(row):\n",
    "    # Determine color based on 'config/optim/anneal_linearly'\n",
    "    if row[\"config/optim/num_epochs\"] == 4:\n",
    "        color = \"#E69F00\"\n",
    "    elif row[\"config/optim/num_epochs\"] == 6:\n",
    "        color = \"#E69F00\"\n",
    "    else:\n",
    "        color = \"#E69F00\"\n",
    "\n",
    "    # Determine the directory key to use based on the presence of 'config/solve_dir'\n",
    "    directory_key = \"config/solve_dir\" if \"config/solve_dir\" in row else \"config/working_dir\"\n",
    "\n",
    "    # Determine shape based on a keyword in the selected directory\n",
    "    directory_value = row[directory_key]\n",
    "    if \"shared-trunk\" in directory_value:\n",
    "        shape = \"s\"  # Circle\n",
    "    elif \"optimizer\" in directory_value:\n",
    "        shape = \"p\"\n",
    "    elif \"regularize-all-layers\" in directory_value:\n",
    "        shape = \"P\"\n",
    "    elif \"regularize\" in directory_value:\n",
    "        shape = \"D\"\n",
    "    else:\n",
    "        shape = \"o\"  # Square as default\n",
    "\n",
    "    return color, shape\n",
    "\n",
    "\n",
    "def plot_correlation(df, df2, metric_key1, metric_key2, metrics_key3, metric_keys, text1, texts2, log=0):\n",
    "    names = df[\"config/env/name\"].drop_duplicates().values\n",
    "    i = 0\n",
    "    for name in names:\n",
    "        mask = df[\"config/env/name\"] == name\n",
    "        # Apply the mask to the DataFrame to get a filtered DataFrame\n",
    "        df_filtered = df.loc[mask]\n",
    "        mask2 = df2[\"config/env/name\"] == name\n",
    "        df_filtered2 = df2.loc[mask2]\n",
    "        # Create a figure with subplots\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=len(metric_keys), figsize=(20, 6))\n",
    "        i = 0\n",
    "        for ax, x_metric in zip(axes, metric_keys):\n",
    "            # Use the average_metrics_per_config_hns function to get data for each metric comparison\n",
    "            points1, points2, colors, shapes = average_metrics_correlation(\n",
    "                df_filtered, metric_key1, metric_key2, x_metric\n",
    "            )\n",
    "\n",
    "            plot_correlation_single_metric(ax, points2, points1, colors, shapes, texts2[i], text1, log)\n",
    "            if i % 3 == 0:\n",
    "                legend_properties = {\"weight\": \"bold\"}\n",
    "                if name != \"\":\n",
    "                    pass\n",
    "            i += 1\n",
    "    axes[0].text(0.02, 0.05, f\"{name}\", fontsize=17, fontweight=\"bold\", transform=axes[0].transAxes)\n",
    "    # add_legends_to_last_subplot2(axes[-1])\n",
    "    plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "    check = names[0].split(\"/\")\n",
    "    if len(check) == 1:\n",
    "        f_name = check[0]\n",
    "    else:\n",
    "        f_name = check[1]\n",
    "    activation = df[\"config/models/activation\"].drop_duplicates().values[0]\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/correlation_by_maps_average_prob_ratio_{f_name}_{activation}.pdf\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def average_metrics_correlation(df, metric_key1, metric_key2, metric_key3):\n",
    "    config_keys = [key for key in df.columns if key.startswith(\"config/\")]\n",
    "    unique_configs = df[config_keys].drop_duplicates()\n",
    "\n",
    "    points_combined_metric = []\n",
    "    points_metric3 = []\n",
    "    colors = []\n",
    "    shapes = []\n",
    "    lowest_metric1_values = []\n",
    "    lowest_metric3_values = []\n",
    "    for _, config_row in unique_configs.iterrows():\n",
    "        mask = create_mask_for_config(df, config_row, config_keys)\n",
    "        filtered_df = df[mask].copy()\n",
    "        color, shape = determine_color_shape2(config_row)\n",
    "\n",
    "        valid_rows = filtered_df.dropna(subset=[metric_key1])\n",
    "        valid_rows.sort_values(by=\"global_step\", ascending=True, inplace=True)\n",
    "        if unique_configs[\"config/env/name\"].values[0].split(\"-\")[-1] == \"v5\":\n",
    "            # ALE.\n",
    "            window_size = 1000000  # Global steps window size\n",
    "            step_size = 1000000  # Step size for moving the window\n",
    "        else:\n",
    "            # MuJoCo\n",
    "            window_size = 50000  # Global steps window size\n",
    "            step_size = 50000  # Step size for moving the window\n",
    "        window_averages = []\n",
    "        window_positions = []\n",
    "        second_metric = []\n",
    "        for start_step in range(0, valid_rows[\"global_step\"].max(), step_size):\n",
    "            window_df = valid_rows[\n",
    "                (valid_rows[\"global_step\"] >= start_step) & (valid_rows[\"global_step\"] < start_step + window_size)\n",
    "            ]\n",
    "            if not window_df.empty:\n",
    "                avg_metric1 = window_df[metric_key1].mean()\n",
    "                avg_metric3 = window_df[metric_key3].mean()\n",
    "                window_averages.append(avg_metric1)\n",
    "                second_metric.append(avg_metric3)\n",
    "                window_positions.append(start_step)\n",
    "\n",
    "        # Find the 20 lowest window averages for metric_key1\n",
    "        if len(window_averages) > 20:\n",
    "            indices_of_lowest = np.argsort(window_averages)[:20]\n",
    "            lowest_metric1_values.extend([window_averages[index] for index in indices_of_lowest])\n",
    "            lowest_metric3_values.extend([second_metric[index] for index in indices_of_lowest])\n",
    "            colors.extend([color] * 20)\n",
    "            shapes.extend([shape] * 20)\n",
    "        else:\n",
    "            lowest_metric1_values.extend(window_averages)\n",
    "            lowest_metric3_values.extend(second_metric)\n",
    "            colors.extend([color] * len(window_averages))\n",
    "            shapes.extend([shape] * len(window_averages))\n",
    "    return lowest_metric1_values, lowest_metric3_values, colors, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a173564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure4_atari(name):\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [name],\n",
    "        (\"config\", \"working_dir\"): [\"baselines\"],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): False,\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [4, 6, 8],\n",
    "    }\n",
    "    df_games = filter_df_by_criteria(atari_df, criteria)\n",
    "\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [name],\n",
    "        (\"config\", \"solve_dir\"): [\"baselines\"],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): False,\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [4, 6, 8],\n",
    "    }\n",
    "    df_games_2 = filter_df_by_criteria(atari_df_capacity, criteria)\n",
    "\n",
    "    plot_correlation(\n",
    "        df_games,\n",
    "        df_games_2,\n",
    "        \"batch/diff/avg_prob_ratio_below_epsilon\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        [\n",
    "            \"batch/start/dead_neurons/features_policy_batch\",\n",
    "            \"batch/start/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "            \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        ],\n",
    "        r\"Avg of prob ratios < 1 - $\\epsilon$\",\n",
    "        [\"Dead neurons policy\", \"Feature rank policy (PCA)\", \"Feature preactivation norm\"],\n",
    "        log=0,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b83589",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure4_atari(\"ALE/Phoenix-v5\")\n",
    "figure4_atari(\"ALE/NameThisGame-v5\")\n",
    "figure4_atari(\"ALE/Qbert-v5\")\n",
    "figure4_atari(\"ALE/Gravitar-v5\")\n",
    "figure4_atari(\"ALE/BattleZone-v5\")\n",
    "figure4_atari(\"ALE/DoubleDunk-v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beea90b",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb94650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure5(num, suffix):\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    N_layers = 1\n",
    "\n",
    "    # Define the model\n",
    "    class PolicyNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(PolicyNetwork, self).__init__()\n",
    "            self.linear = nn.Linear(N_layers, 2, bias=False)  # Single state input, two actions output\n",
    "\n",
    "        def forward(self, state):\n",
    "            logits = self.linear(state)\n",
    "            return nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model = PolicyNetwork()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1.5)\n",
    "\n",
    "    # Example state x and alpha\n",
    "    feature_norm = 1\n",
    "    x = feature_norm * torch.randn((1, N_layers), dtype=torch.float)\n",
    "    y = feature_norm * torch.randn((1, N_layers), dtype=torch.float)\n",
    "\n",
    "    copy_n = N_layers\n",
    "    alpha = num  # -2 for interference, 3 for boost\n",
    "    y[:, :copy_n] = alpha * x[:, :copy_n]\n",
    "\n",
    "    # Fixed advantage and old policy probabilities for demonstration\n",
    "    A = 1.0  # Advantage\n",
    "    old_pi_a1_x = model(x)[0, 0].item()\n",
    "    old_pi_a1_y = model(y)[0, 0].item()\n",
    "    epsilon = 0.1\n",
    "\n",
    "    color = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]\n",
    "\n",
    "    def compute_loss(prob_a1, old_prob_a1):\n",
    "        \"\"\"assume A > 0 for simplicity\"\"\"\n",
    "        ratio = prob_a1 / old_prob_a1\n",
    "        clipped_ratio = torch.clamp(ratio, max=1 + epsilon)\n",
    "        return A * torch.min(ratio, clipped_ratio)\n",
    "\n",
    "    # Training loop with alternating updates\n",
    "    ratio_x_history = [1]\n",
    "    ratio_y_history = [1]\n",
    "\n",
    "    steps = 20\n",
    "    for step in range(steps):\n",
    "        # Alternate between x and y\n",
    "        state = x if step % 2 == 0 else y\n",
    "        old_pi_a1 = old_pi_a1_x if step % 2 == 0 else old_pi_a1_y\n",
    "\n",
    "        # Forward pass\n",
    "        probs = model(state)\n",
    "        prob_a1 = probs[0, 0]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = -compute_loss(prob_a1, old_pi_a1)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log probabilities for plotting\n",
    "        with torch.no_grad():\n",
    "            pi_x = model(x)[0, 0].item()\n",
    "            pi_y = model(y)[0, 0].item()\n",
    "            ratio_x = pi_x / old_pi_a1_x\n",
    "            ratio_y = pi_y / old_pi_a1_y\n",
    "            ratio_x_history.append(ratio_x)\n",
    "            ratio_y_history.append(ratio_y)\n",
    "            # Uncomment if you want to have the step by step printed.\n",
    "            # print(f\"Step {step}: pi_x = {pi_x:.2f}, pi_y = {pi_y:.2f}, ratio_x = {ratio_x:.2f}, ratio_y = {ratio_y:.2f}\")\n",
    "    # Plotting\n",
    "    # Set the y-axis between 0 and 2\n",
    "    # plt.ylim(0, 2)\n",
    "    plt.plot(ratio_x_history, label=r\"$\\pi_\\theta(a_1| x) / \\pi_\\text{old}(a_1|x)$\", color=\"#E69F00\", linewidth=4)\n",
    "    plt.plot(ratio_y_history, label=r\"$\\pi_\\theta(a_1 | y) / \\pi_\\text{old}(a_1 | y)$\", color=\"#56B4E9\", linewidth=4)\n",
    "    plt.xlabel(\"Minibatch\")\n",
    "    plt.ylabel(\"Ratio\")\n",
    "\n",
    "    # plot the 1+epsilon line\n",
    "    plt.axhline(1 + epsilon, color=\"r\", linestyle=\"--\", label=r\"1+$\\epsilon$\", linewidth=4)\n",
    "\n",
    "    plt.legend(loc=(-0.3, 1), ncol=3, frameon=False, handlelength=1, handletextpad=0.2, columnspacing=1)\n",
    "    # x-axis as integer ticks\n",
    "    # plt.xticks(range(0, steps + 1))\n",
    "    # show grid lines\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/toy_example_{suffix}.pdf\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure5(-2, \"neg\")\n",
    "figure5(3, \"pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c28c0",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3113ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_keywords(df, df2, metrics, metric2, metric_name, keywords, save_name=\"\"):\n",
    "    \"\"\"\n",
    "    Plot box plots for given metrics, aggregating data by keywords found in 'config/working_dir'.\n",
    "\n",
    "    :param df: DataFrame containing the dataset.\n",
    "    :param metrics: List of metric keys to plot.\n",
    "    :param keywords: List of keywords to filter configurations by their working directory.\n",
    "    \"\"\"\n",
    "    key = [\n",
    "        \"Share actor and critic features\",\n",
    "        \"Reset Adam\",\n",
    "        \"Regularize all preactivations\",\n",
    "        \"Regularize last preactivation\",\n",
    "        \"No invervention\",\n",
    "    ]\n",
    "    # Prepare the aggregated data storage\n",
    "    keys = [\n",
    "        \"baselines\",\n",
    "        \"ppo-kl\",\n",
    "        \"ppo-early-stop\",\n",
    "        \"all\",\n",
    "        \"optimizer\",\n",
    "        \"regularize\",\n",
    "        \"regularize-all-layers\",\n",
    "        \"shared-trunk\",\n",
    "    ]\n",
    "\n",
    "    all_metrics = metrics + [metric2]\n",
    "    aggregated_data = {keyword: {metric: [] for metric in all_metrics} for keyword in keywords}\n",
    "\n",
    "    config_keys = [key for key in df.columns if key.startswith(\"config/\")]\n",
    "    unique_configs = df[config_keys].drop_duplicates()\n",
    "    colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]\n",
    "    return_nb = []\n",
    "    N_nb = []\n",
    "    return_steps = []\n",
    "    ratio_nb = []\n",
    "    ratio_N_nb = []\n",
    "    ratio_steps = []\n",
    "    return_info = []\n",
    "    ratio_info = []\n",
    "    ratio_res = []\n",
    "    for _, config_row in tqdm(unique_configs.iterrows(), total=len(unique_configs)):\n",
    "        mask = create_mask_for_config(df, config_row, config_keys)\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        for keyword in keywords:\n",
    "            if \"algo\" in config_row[\"config/working_dir\"]:\n",
    "                continue\n",
    "            else:\n",
    "                if keyword in config_row[\"config/working_dir\"]:\n",
    "                    for metric in metrics:\n",
    "                        if metric == \"ProbRatio_\":\n",
    "                            metric_key1 = \"batch/diff/avg_prob_ratio_above_epsilon\"\n",
    "                            metric_key2 = \"batch/diff/avg_prob_ratio_below_epsilon\"\n",
    "\n",
    "                            # Remove rows where either metric_key1 or metric_key2 is NaN\n",
    "                            df_no_nan = filtered_df.dropna(subset=[metric_key1, metric_key2])\n",
    "                            df_sorted = df_no_nan.sort_values(by=\"global_step\", ascending=True)  # Sort by 'global_step'\n",
    "                            B_max = df_sorted.index.max()\n",
    "                            # Get max 'global_step' before and after dropping NaNs for verification\n",
    "                            unfiltered_max = filtered_df[\"global_step\"].max()\n",
    "                            filtered_max = df_sorted[\"global_step\"].max()\n",
    "\n",
    "                            df_max = df_no_nan.sort_values(by=\"global_step\", ascending=False)[\"global_step\"].unique()\n",
    "                            idx_max = 0\n",
    "\n",
    "                            five_percent_of_unfiltered_max = int(unfiltered_max * 0.05)\n",
    "\n",
    "                            # Initial threshold calculation\n",
    "                            threshold_step = filtered_max - five_percent_of_unfiltered_max\n",
    "                            first_index_above = df_sorted[df_sorted[\"global_step\"] >= threshold_step].index.min()\n",
    "                            B_min = first_index_above\n",
    "                            result_df = df_sorted.loc[B_min:]\n",
    "                            while result_df[metric_key1].count() < 10:\n",
    "                                idx_max += 1\n",
    "                                threshold_step = df_max[idx_max] - five_percent_of_unfiltered_max\n",
    "                                # Recalculate B_min based on the new threshold\n",
    "                                first_index_above = df_sorted[df_sorted[\"global_step\"] >= threshold_step].index.min()\n",
    "\n",
    "                                B_min = first_index_above\n",
    "                                B_max = df_sorted[\n",
    "                                    df_sorted[\"global_step\"] <= df_max[idx_max]\n",
    "                                ].index.max()  # Redefine B_max to ensure it stays within the new filtered_max\n",
    "\n",
    "                                # Update result_df with the new range\n",
    "                                result_df = df_sorted.loc[B_min:B_max]\n",
    "\n",
    "                                # If filtered_max reaches the minimum global_step or no further reduction is possible, break the loop\n",
    "                                if filtered_max <= df_sorted[\"global_step\"].min():\n",
    "                                    break\n",
    "\n",
    "                            N = B_max - B_min + 1\n",
    "\n",
    "                            # Store additional info if needed\n",
    "                            ratio_steps.append((result_df[\"global_step\"].min(), result_df[\"global_step\"].max()))\n",
    "\n",
    "                            ratio_info.append(\n",
    "                                (\n",
    "                                    df_sorted[\"config/optim/num_epochs\"].values[0],\n",
    "                                    df_sorted[\"config/working_dir\"].values[0],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                            # Prepare and calculate combined metric\n",
    "                            f_1 = result_df[metric_key1]\n",
    "                            f_2 = result_df[metric_key2]\n",
    "                            ratio_nb.append((result_df[metric_key1].count(), result_df[metric_key2].count()))\n",
    "                            ratio_N_nb.append(N)\n",
    "                            combined_metric = (f_1 / f_2).mean()\n",
    "                            # combined_metric = np.clip((f_1 / f_2).mean(), a_min=-np.inf,a_max=2.75)\n",
    "                            ratio_res.append(combined_metric)\n",
    "                            aggregated_data[keyword][metric].append(combined_metric)\n",
    "\n",
    "                        else:\n",
    "                            if metric == \"batch/perf/avg_return_raw\":\n",
    "                                threshold_step = filtered_df[\"global_step\"].max() * 0.95  # Calculate the 95% threshold\n",
    "\n",
    "                                df_sorted = filtered_df.sort_values(by=\"global_step\", ascending=True)\n",
    "\n",
    "                                first_index_above = df_sorted[\n",
    "                                    df_sorted[\"global_step\"] >= threshold_step\n",
    "                                ].index.min()  # Find the first index where 'global_step' exceeds the threshold\n",
    "                                B_min = first_index_above\n",
    "                                B_max = df_sorted.index.max()\n",
    "                                N = B_max - B_min + 1\n",
    "\n",
    "                                result_df = df_sorted.loc[B_min:]  # Select this and all subsequent batches\n",
    "                                return_steps.append((result_df[\"global_step\"].min(), result_df[\"global_step\"].max()))\n",
    "                                final_rows = []\n",
    "                                removed_count = 0\n",
    "                                last_timestep = float(\"inf\")\n",
    "                                return_info.append(\n",
    "                                    (\n",
    "                                        df_sorted[\"config/optim/num_epochs\"].values[0],\n",
    "                                        df_sorted[\"config/working_dir\"].values[0],\n",
    "                                    )\n",
    "                                )\n",
    "                                for _, row in result_df.iterrows():\n",
    "                                    # Handling duplicates due to redundant in logging.\n",
    "                                    if not final_rows:\n",
    "                                        final_rows.append(row)  # Add the first row automatically\n",
    "                                        last_timestep = row[\"batch/perf/max_timestep\"]\n",
    "                                    else:\n",
    "                                        last_row = final_rows[-1]  # Check the last point added\n",
    "                                        if (\n",
    "                                            row[\"batch/perf/avg_return_raw\"] == last_row[\"batch/perf/avg_return_raw\"]\n",
    "                                        ):  # if duplicate\n",
    "                                            if (\n",
    "                                                row[\"batch/perf/max_timestep\"] <= last_timestep or removed_count >= 8\n",
    "                                            ):  # if we have already removed 8 points (duplicates) or if the max time step is lower than the previous one\n",
    "                                                removed_count = 0  # Reset the count\n",
    "                                                final_rows.append(row)  # Add this row as a valid point after reset\n",
    "                                            else:\n",
    "                                                removed_count += 1  # Else Increment removed count for duplicates, we are still in the episode\n",
    "                                        else:\n",
    "                                            # Add to final rows if it's not a duplicate\n",
    "                                            final_rows.append(row)\n",
    "                                            removed_count = 0  # Reset the count since a valid point was added\n",
    "                                        last_timestep = row[\"batch/perf/max_timestep\"]\n",
    "                                return_nb.append(len(final_rows))\n",
    "                                N_nb.append(N)\n",
    "\n",
    "                                final_df = pd.DataFrame(final_rows).sort_values(by=\"global_step\", ascending=False)\n",
    "\n",
    "                                mean_value = final_df[\n",
    "                                    \"batch/perf/avg_return_raw\"\n",
    "                                ].mean()  # Calculate the mean of the metric\n",
    "\n",
    "                                # Storing the result\n",
    "                                aggregated_data[keyword][metric].append(mean_value)\n",
    "\n",
    "                            else:\n",
    "                                threshold_step = filtered_df[\"global_step\"].max() * 0.95  # Calculate the 95% threshold\n",
    "\n",
    "                                df_sorted = filtered_df.sort_values(by=\"global_step\", ascending=True)\n",
    "\n",
    "                                first_index_above = df_sorted[\n",
    "                                    df_sorted[\"global_step\"] >= threshold_step\n",
    "                                ].index.min()  # Find the first index where 'global_step' exceeds the threshold\n",
    "                                B_min = first_index_above\n",
    "                                B_max = df_sorted.index.max()\n",
    "                                N = B_max - B_min + 1\n",
    "\n",
    "                                result_df = df_sorted.loc[B_min:]  # Select this and all subsequent batches\n",
    "\n",
    "                                mean_value = result_df[metric].mean()  # Calculate the mean of the metric\n",
    "\n",
    "                                aggregated_data[keyword][metric].append(mean_value)  # Storing the result\n",
    "\n",
    "    config_keys2 = [key for key in df2.columns if key.startswith(\"config/\")]\n",
    "    unique_configs2 = df2[config_keys2].drop_duplicates()\n",
    "    i = 0\n",
    "\n",
    "    for _, config_row2 in unique_configs2.iterrows():\n",
    "        mask2 = create_mask_for_config(df2, config_row2, config_keys2)\n",
    "        filtered_df2 = df2[mask2].copy()\n",
    "\n",
    "        for keyword in keywords:\n",
    "            if \"algo\" in config_row2[\"config/solve_dir\"]:\n",
    "                continue\n",
    "            else:\n",
    "                if keyword in config_row2[\"config/solve_dir\"]:\n",
    "                    metric = \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\"\n",
    "                    threshold_step = (\n",
    "                        filtered_df2[\"capacity-counters/env_steps\"].max() * 0.95\n",
    "                    )  # Calculate the 95% threshold\n",
    "\n",
    "                    df_sorted = filtered_df2.sort_values(by=\"capacity-counters/env_steps\", ascending=True)\n",
    "\n",
    "                    first_index_above = df_sorted[\n",
    "                        df_sorted[\"capacity-counters/env_steps\"] > threshold_step\n",
    "                    ].index.min()  # Find the first index where 'global_step' exceeds the threshold\n",
    "                    B_min = first_index_above - 1\n",
    "                    B_max = df_sorted.index.max()\n",
    "                    N = B_max - B_min + 1\n",
    "\n",
    "                    result_df = df_sorted.loc[B_min:]  # Select this and all subsequent batches\n",
    "\n",
    "                    mean_value = result_df[metric].mean()  # Calculate the mean of the metric\n",
    "\n",
    "                    aggregated_data[keyword][metric].append(mean_value)  # Storing the result\n",
    "\n",
    "    # Uncomment if you want information about each point in the boxplot (return and prob ratio)\n",
    "\n",
    "    #     print(save_name.split(\"-\")[2])\n",
    "    #     print(f\"- Return : \\n\")\n",
    "    #     for i in range(len(N_nb)):\n",
    "    #         print(f\"{return_steps[i][0]} -> {return_steps[i][1]}, {return_nb[i]}/{N_nb[i]}, {return_info[i][0]},{return_info[i][1]}\")\n",
    "    #     print(f\"- Ratio : \\n\")\n",
    "    #     for i in range(len(ratio_N_nb)):\n",
    "    #         print(f\"{ratio_steps[i][0]} -> {ratio_steps[i][1]}, {ratio_nb[i]}/{ratio_N_nb[i]}, {ratio_info[i][0]},{ratio_info[i][1]}, {ratio_res[i]}\")\n",
    "\n",
    "    meanlineprops = dict(linestyle=\"-.\", linewidth=3, color=\"red\")\n",
    "    medianprops = dict(linestyle=\"-.\", linewidth=3, color=\"black\")\n",
    "\n",
    "    # Assuming all_metrics, aggregated_data, keywords, medianprops, meanlineprops, flierprops, and colors are defined\n",
    "    fig = plt.figure(figsize=(20, 2))\n",
    "    gs = plt.GridSpec(1, len(all_metrics), figure=fig)  # GridSpec for the entire figure\n",
    "\n",
    "    axes = []  # List to store either normal or broken axes\n",
    "\n",
    "    for col_idx, metric in enumerate(all_metrics):\n",
    "        m = [aggregated_data[keyword][metric] for keyword in keywords]\n",
    "        prob_means = [np.mean(aggregated_data[keyword][\"ProbRatio_\"]) for keyword in keywords]\n",
    "        g_min = min(np.min(data) for data in m)\n",
    "        g_max = max(np.max(data) for data in m)\n",
    "        print(f\"prob means: {prob_means}\")\n",
    "        if metric == \"ProbRatio_\" and g_max > 2.5:\n",
    "            bax = brokenaxes(\n",
    "                xlims=[(g_min - 0.1, 2.5), (g_max - 0.1, g_max + 0.3)],\n",
    "                subplot_spec=gs[0, col_idx],\n",
    "                fig=fig,\n",
    "                despine=False,\n",
    "            )\n",
    "            for s in bax.axs:  # axs is a list of the sub-axes of the broken axes\n",
    "                s.tick_params(axis=\"y\", which=\"both\", left=False, labelleft=False)\n",
    "                for tick in s.yaxis.get_major_ticks():\n",
    "                    tick.tick1line.set_visible(False)  # Hides the inner tick lines\n",
    "                    tick.tick2line.set_visible(False)  # Hides the outer tick lines\n",
    "                # If you have minor ticks enabled\n",
    "                for tick in s.yaxis.get_minor_ticks():\n",
    "                    tick.tick1line.set_visible(False)\n",
    "                    tick.tick2line.set_visible(False)\n",
    "            reduction_factor = 0.3\n",
    "            for line in bax.diag_handles:\n",
    "                # Get the current data of the line\n",
    "                xdata, ydata = line.get_data()\n",
    "\n",
    "                # Calculate new data points by reducing the length by the specified factor\n",
    "                # This moves the start and end points closer to the midpoint of the line\n",
    "                mid_x = np.mean(xdata)\n",
    "                mid_y = np.mean(ydata)\n",
    "                new_xdata = mid_x + reduction_factor * (xdata - mid_x)\n",
    "                new_ydata = mid_y + reduction_factor * (ydata - mid_y)\n",
    "\n",
    "                # Set the new data for the line\n",
    "                line.set_data(new_xdata, new_ydata)\n",
    "                line.set_linewidth(1)\n",
    "            ax = bax\n",
    "            ax.set_yticklabels([])\n",
    "        else:\n",
    "            # Regular axes\n",
    "            ax = fig.add_subplot(gs[0, col_idx])\n",
    "            if metric in [\"batch/end/feature_stats/norm_features_preactivation_policy_batch\"]:\n",
    "                ax.set_xscale(\"log\")\n",
    "            if metric == \"ProbRatio_\" and save_name.split(\"-\")[2] == \"Gravitar\":\n",
    "                ax.set_xscale(\"log\")\n",
    "\n",
    "        axes.append(ax)  # Append the created axis, either broken or regular, to the list\n",
    "\n",
    "        # Create the boxplot\n",
    "        bp = ax.boxplot(\n",
    "            m,\n",
    "            widths=0.6,\n",
    "            patch_artist=True,\n",
    "            vert=False,\n",
    "            medianprops=medianprops,\n",
    "            meanprops=meanlineprops,\n",
    "            showmeans=True,\n",
    "            meanline=True,\n",
    "            flierprops=dict(\n",
    "                marker=\"o\", markerfacecolor=\"none\", markeredgecolor=\"black\", markersize=8, markeredgewidth=2\n",
    "            ),\n",
    "        )\n",
    "        # Color the boxes and set titles\n",
    "        if metric == \"ProbRatio_\" and g_max > 2.5:\n",
    "            bp = bp[0]\n",
    "        for idx, patch in enumerate(bp[\"boxes\"]):\n",
    "            patch.set_facecolor(colors[idx % len(colors)])  # Use modulo for color cycling\n",
    "\n",
    "        ax.set_title(f\"{metric_name[col_idx]}\", fontsize=13.5)\n",
    "\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        if col_idx >= 0 and isinstance(ax, plt.Axes):\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_yticks([])\n",
    "        if col_idx == 0:\n",
    "            ax.set_yticks([y + 1 for y in range(len(m))], labels=key)\n",
    "\n",
    "    mean_line = lines.Line2D([], [], color=\"red\", linestyle=\"-.\", linewidth=3, label=\"Mean\")\n",
    "    median_line = lines.Line2D([], [], color=\"black\", linestyle=\"-.\", linewidth=3, label=\"Median\")\n",
    "    outlier_marker = lines.Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        label=\"Outliers\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=\"black\",\n",
    "        markersize=8,\n",
    "        markeredgewidth=2,\n",
    "    )\n",
    "    axes[-1].legend(\n",
    "        handles=[mean_line, median_line, outlier_marker],\n",
    "        loc=\"upper center\",  # This anchors the center of the legend at the provided coordinate\n",
    "        bbox_to_anchor=(-2.8, 1.4),  # Anchors the legend above the plot\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=12,\n",
    "        ncol=3,\n",
    "        frameon=False,\n",
    "        handlelength=1,\n",
    "        handletextpad=0.5,\n",
    "        columnspacing=1,\n",
    "    )\n",
    "    fig.text(-0.015, 0.01, f'{save_name.split(\"-\")[2]}', ha=\"left\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/boxplot-{save_name}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure6_atari(name):\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "    }\n",
    "    df_check = filter_df_by_criteria(atari_df, criteria)\n",
    "    criteria2 = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "    }\n",
    "    f_name = f\"6-{name}\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    df_check2 = filter_df_by_criteria(atari_df_capacity, criteria2)\n",
    "    config_keywords = [\"shared-trunk\", \"optimizer\", \"regularize-all-layers\", \"regularize\", \"baselines\"]\n",
    "    metrics = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"ProbRatio_\",\n",
    "        \"batch/end/dead_neurons/features_policy_batch\",\n",
    "        \"batch/end/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "    ]\n",
    "    metric_name = [\n",
    "        \"Episode return\",\n",
    "        \"Excess ratio\",\n",
    "        \"Dead neurons policy\",\n",
    "        \"Norm preactivation policy\",\n",
    "        \"Feature rank policy (PCA)\",\n",
    "        \"Plasticity loss policy\",\n",
    "    ]\n",
    "    plot_metrics_by_keywords(\n",
    "        df_check,\n",
    "        df_check2,\n",
    "        metrics,\n",
    "        \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "        metric_name,\n",
    "        config_keywords,\n",
    "        save_name=f_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure6_atari(\"ALE/Gravitar-v5\")\n",
    "figure6_atari(\"ALE/Phoenix-v5\")\n",
    "figure6_atari(\"ALE/NameThisGame-v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492288dd",
   "metadata": {},
   "source": [
    "## Correlation between the 5 ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(y1, y2):\n",
    "    \"\"\"\n",
    "    Calculate the discrete Fréchet distance between two curves represented by their y-values,\n",
    "    assuming they share the same x-coordinates.\n",
    "    \"\"\"\n",
    "    # Compute the L2 distance\n",
    "    l2_distance = np.linalg.norm(y1 - y2)\n",
    "\n",
    "    # Normalize the L2 distance\n",
    "    normalized_distance = l2_distance / (np.sqrt(len(y1)) * 512)\n",
    "\n",
    "    # The value at the bottom-right corner is the total distance\n",
    "    return normalized_distance\n",
    "\n",
    "\n",
    "def average_correlations_ranks(df, metrics_keys):\n",
    "    config_keys = [key for key in df.columns if key.startswith(\"config/\")]\n",
    "    unique_configs = df[config_keys].drop_duplicates()\n",
    "\n",
    "    kendall_sums = np.zeros((len(metrics_keys), len(metrics_keys)))\n",
    "    spearman_sums = np.zeros((len(metrics_keys), len(metrics_keys)))\n",
    "    pearson_sums = np.zeros((len(metrics_keys), len(metrics_keys)))\n",
    "    distance_sums = np.zeros((len(metrics_keys), len(metrics_keys)))\n",
    "\n",
    "    total_configs = len(unique_configs)\n",
    "    processed_configs = 0\n",
    "\n",
    "    for _, config_row in tqdm(unique_configs.iterrows(), total=len(unique_configs)):\n",
    "        mask = create_mask_for_config(df, config_row, config_keys)\n",
    "        filtered_df2 = df[mask]\n",
    "\n",
    "        # Preliminary check for constant metrics\n",
    "        has_constant_metric = False\n",
    "        for metric in metrics_keys:\n",
    "            y = filtered_df2[metric].dropna().values  # Drop NaN to get valid values\n",
    "            if len(np.unique(y)) <= 1:  # Check if the metric is constant\n",
    "                has_constant_metric = True\n",
    "                break  # Exit the loop as we found a constant metric\n",
    "\n",
    "        if has_constant_metric:\n",
    "            continue  # Skip to the next configuration if a constant metric is found\n",
    "        processed_configs += 1\n",
    "        # If no constant metrics, proceed with calculations\n",
    "        for i, metric1 in enumerate(metrics_keys):\n",
    "            for j, metric2 in enumerate(metrics_keys):\n",
    "                if i > j:  # Skip redundant calculations and self-comparison\n",
    "                    kendall_sums[i, j] = kendall_sums[j, i]\n",
    "                    spearman_sums[i, j] = spearman_sums[j, i]\n",
    "                    pearson_sums[i, j] = pearson_sums[j, i]\n",
    "                    distance_sums[i, j] = distance_sums[j, i]\n",
    "                    continue\n",
    "                y1 = filtered_df2[metric1].values\n",
    "                y2 = filtered_df2[metric2].values\n",
    "                if len(y1) != len(y2):\n",
    "                    print(filtered_df2[metric1].values)\n",
    "                    print(filtered_df2[metric2].values)\n",
    "                valid_indices = ~np.isnan(y1) & ~np.isnan(y2)\n",
    "                y1, y2 = y1[valid_indices], y2[valid_indices]\n",
    "\n",
    "                if len(np.unique(y1)) > 1 and len(np.unique(y2)) > 1:\n",
    "                    kendall_sums[i, j] += kendalltau(y1, y2)[0]\n",
    "                    spearman_sums[i, j] += spearmanr(y1, y2)[0]\n",
    "                    pearson_sums[i, j] += pearsonr(y1, y2)[0] if len(y1) > 1 else np.nan\n",
    "                    # Assume frechet_dist function is defined\n",
    "                    distance_sums[i, j] += 1 - distance(y1, y2)\n",
    "\n",
    "        # Increment the count of processed configurations\n",
    "        completion_percentage = (processed_configs / total_configs) * 100\n",
    "    #         print(f\"Processed {processed_configs}/{total_configs} configurations ({completion_percentage:.2f}%)\")\n",
    "\n",
    "    kendall_avg = kendall_sums / processed_configs\n",
    "    spearman_avg = spearman_sums / processed_configs\n",
    "    pearson_avg = pearson_sums / processed_configs\n",
    "    distance_avg = distance_sums / processed_configs\n",
    "\n",
    "    correlations = {\n",
    "        \"Kendall Tau\": kendall_avg,\n",
    "        \"Spearman Rho\": spearman_avg,\n",
    "        \"Pearson\": pearson_avg,\n",
    "        \"1-distance\": distance_avg,\n",
    "    }\n",
    "\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrices(correlations, metrics_keys, name_figure=None):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(40, 30))  # Adjust figsize as needed\n",
    "    axs = axs.flatten()  # Flatten to easily iterate over\n",
    "    plt.subplots_adjust(wspace=0.45, hspace=0.4)  # Adjust space between plots\n",
    "\n",
    "    for i, (title, matrix) in enumerate(correlations.items()):\n",
    "        df_matrix = pd.DataFrame(matrix, index=metrics_keys, columns=metrics_keys)\n",
    "        row_averages = df_matrix.mean(axis=1)\n",
    "        row_std_devs = df_matrix.std(axis=1)\n",
    "        df_extended = df_matrix.copy()\n",
    "        df_extended[\"Avg\"] = row_averages\n",
    "        df_extended[\"Std\"] = np.nan\n",
    "\n",
    "        pos = axs[i].get_position()\n",
    "        cbar_width = 0.02\n",
    "        cbar_height = 0.3\n",
    "        cbar_xoffset = 0.03\n",
    "        cbar_ax = fig.add_axes(\n",
    "            [pos.x1 + cbar_xoffset, pos.y0 + (pos.height - cbar_height) / 2, cbar_width, cbar_height]\n",
    "        )\n",
    "        if i == 3:\n",
    "            heatmap = sns.heatmap(\n",
    "                df_extended,\n",
    "                ax=axs[i],\n",
    "                cmap=\"coolwarm\",\n",
    "                annot_kws={\"size\": 35},\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cbar_ax=cbar_ax,\n",
    "                square=False,\n",
    "            )\n",
    "        else:\n",
    "            heatmap = sns.heatmap(\n",
    "                df_extended,\n",
    "                ax=axs[i],\n",
    "                cmap=\"coolwarm\",\n",
    "                annot_kws={\"size\": 35},\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cbar_ax=cbar_ax,\n",
    "                square=False,\n",
    "            )\n",
    "        cbar = heatmap.collections[0].colorbar\n",
    "        cbar.ax.tick_params(labelsize=25)\n",
    "        facecolors = heatmap.collections[0].get_facecolors()\n",
    "        avg_column_index = len(metrics_keys)  # 'Avg' is next to the last original metric column\n",
    "        n_columns_incl_avg = len(metrics_keys) + 2\n",
    "\n",
    "        for y in range(df_matrix.shape[0]):\n",
    "            color_index = y * n_columns_incl_avg + avg_column_index\n",
    "            color = facecolors[color_index]\n",
    "\n",
    "            # Draw rectangle for 'Std' with the same color as 'Avg'\n",
    "            axs[i].add_patch(plt.Rectangle((6, y), 1, 1, fill=True, facecolor=color, edgecolor=\"none\"))\n",
    "            axs[i].add_patch(plt.Rectangle((5, y), 1, 1, fill=True, facecolor=\"white\", edgecolor=\"none\"))\n",
    "            if i == 3:\n",
    "                # Annotate 'Std' value over the rectangle\n",
    "                axs[i].text(\n",
    "                    avg_column_index + 1.5,\n",
    "                    y + 0.5,\n",
    "                    f\"{row_averages.iloc[y]:.2f}\\n±{row_std_devs.iloc[y]:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"black\",\n",
    "                    fontsize=35,\n",
    "                )\n",
    "            else:\n",
    "                axs[i].text(\n",
    "                    avg_column_index + 1.5,\n",
    "                    y + 0.5,\n",
    "                    f\"{row_averages.iloc[y]:.2f}\\n±{row_std_devs.iloc[y]:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"black\",\n",
    "                    fontsize=35,\n",
    "                )\n",
    "            axs[i].text(\n",
    "                5.5,\n",
    "                y + 0.5,\n",
    "                f\"{row_averages.iloc[y]:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\",\n",
    "                fontsize=35,\n",
    "                bbox=dict(facecolor=\"white\", edgecolor=\"none\", pad=10),\n",
    "            )\n",
    "\n",
    "        axs[i].set_title(title, fontsize=35)\n",
    "\n",
    "        text = [\"Vetterli\", \"PCA\", \"Kumar\", \"Lyle\", \"PyTorch\"]\n",
    "        tick_labels = text + [\"\", \"Avg ± Std\"]\n",
    "        axs[i].set_xticks(np.arange(len(tick_labels)) + 0.5)\n",
    "        axs[i].set_xticklabels(tick_labels, rotation=90, ha=\"center\", fontsize=30)\n",
    "        axs[i].set_yticks(np.arange(len(metrics_keys)) + 0.5)\n",
    "        text = [\"Vetterli\", \"PCA\", \"Kumar\", \"Lyle\", \"Pytorch\"]\n",
    "        axs[i].set_yticklabels(text, fontsize=30, va=\"center\")\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/correlation_{name_figure}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3afbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_avg_ranks_atari():\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            \"ALE/Phoenix-v5\",\n",
    "            \"ALE/NameThisGame-v5\",\n",
    "            \"ALE/DoubleDunk-v5\",\n",
    "            \"ALE/Gravitar-v5\",\n",
    "            \"ALE/BattleZone-v5\",\n",
    "            \"ALE/Qbert-v5\",\n",
    "        ],\n",
    "        (\"config\", \"working_dir\"): \"baselines\",\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "    }\n",
    "\n",
    "    filtered_df = filter_df_by_criteria(atari_df, criteria)\n",
    "\n",
    "    metrics_keys2 = [\n",
    "        \"batch/end/SVD/effective_rank_vetterli/features_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/end/SVD/srank_kumar/features_policy_batch\",\n",
    "        \"batch/end/SVD/feature_rank_lyle/features_policy_batch\",\n",
    "        \"batch/end/SVD/pytorch_rank/features_policy_batch\",\n",
    "    ]\n",
    "    correlations = average_correlations_ranks(filtered_df, metrics_keys2)\n",
    "    plot_correlation_matrices(correlations, metrics_keys2, \"avg_atari\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee669af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_avg_ranks_atari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_correlations(df, metrics_keys):\n",
    "    config_keys = [key for key in df.columns if key.startswith(\"config/\")]\n",
    "    unique_configs = df[config_keys].drop_duplicates()\n",
    "\n",
    "    # Initialize with positive infinity to find minimum values\n",
    "    kendall_worst = np.full((len(metrics_keys), len(metrics_keys)), 1.0)\n",
    "    spearman_worst = np.full((len(metrics_keys), len(metrics_keys)), 1.0)\n",
    "    pearson_worst = np.full((len(metrics_keys), len(metrics_keys)), 1.0)\n",
    "    distance_worst = np.full((len(metrics_keys), len(metrics_keys)), 1.0)\n",
    "\n",
    "    total_configs = len(unique_configs)\n",
    "    processed_configs = 0\n",
    "    for _, config_row in tqdm(unique_configs.iterrows(), total=total_configs):\n",
    "        mask = create_mask_for_config(df, config_row, config_keys)\n",
    "        filtered_df2 = df[mask]\n",
    "\n",
    "        # Preliminary check for constant metrics\n",
    "        has_constant_metric = False\n",
    "        for metric in metrics_keys:\n",
    "            y = filtered_df2[metric].dropna().values  # Drop NaN to get valid values\n",
    "            if len(np.unique(y)) <= 1:  # Check if the metric is constant\n",
    "                has_constant_metric = True\n",
    "                break  # Exit the loop as we found a constant metric\n",
    "\n",
    "        if has_constant_metric:\n",
    "            continue  # Skip to the next configuration if a constant metric is found\n",
    "        for i, metric1 in enumerate(metrics_keys):\n",
    "            for j, metric2 in enumerate(metrics_keys):\n",
    "                if i >= j:  # Skip redundant calculations and self-comparison\n",
    "                    continue\n",
    "                y1 = filtered_df2[metric1].values\n",
    "                y2 = filtered_df2[metric2].values\n",
    "                if len(y1) != len(y2):\n",
    "                    print(filtered_df2[metric1].values)\n",
    "                    print(filtered_df2[metric2].values)\n",
    "                valid_indices = ~np.isnan(y1) & ~np.isnan(y2)\n",
    "                y1, y2 = y1[valid_indices], y2[valid_indices]\n",
    "\n",
    "                if len(y1) > 0 and len(y2) > 0:\n",
    "                    kendall_corr = kendalltau(y1, y2)[0]\n",
    "                    spearman_corr = spearmanr(y1, y2)[0]\n",
    "                    pearson_corr = pearsonr(y1, y2)[0] if len(y1) > 1 else np.nan\n",
    "                    # Assume frechet_dist function is defined and compatible\n",
    "                    distance_corr = 1 - distance(y1, y2) if len(y1) > 1 else np.nan\n",
    "                    # Update worst (min) correlation values\n",
    "                    kendall_worst[i, j] = min(kendall_worst[i, j], kendall_corr)\n",
    "                    spearman_worst[i, j] = min(spearman_worst[i, j], spearman_corr)\n",
    "                    pearson_worst[i, j] = (\n",
    "                        min(pearson_worst[i, j], pearson_corr) if not np.isnan(pearson_corr) else pearson_worst[i, j]\n",
    "                    )\n",
    "                    distance_worst[i, j] = (\n",
    "                        min(distance_worst[i, j], distance_corr)\n",
    "                        if not np.isnan(distance_corr)\n",
    "                        else distance_worst[i, j]\n",
    "                    )\n",
    "\n",
    "    # Mirror the lower triangle to the upper for symmetric matrices\n",
    "    for i in range(len(metrics_keys)):\n",
    "        for j in range(i + 1, len(metrics_keys)):\n",
    "            kendall_worst[j, i] = kendall_worst[i, j]\n",
    "            spearman_worst[j, i] = spearman_worst[i, j]\n",
    "            pearson_worst[j, i] = pearson_worst[i, j]\n",
    "            distance_worst[j, i] = distance_worst[i, j]\n",
    "\n",
    "    correlations = {\n",
    "        \"Kendall Tau\": kendall_worst,\n",
    "        \"Spearman Rho\": spearman_worst,\n",
    "        \"Pearson\": pearson_worst,\n",
    "        \"1-distance\": distance_worst,\n",
    "    }\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def figure_worst_ranks_atari():\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            \"ALE/Phoenix-v5\",\n",
    "            \"ALE/NameThisGame-v5\",\n",
    "            \"ALE/DoubleDunk-v5\",\n",
    "            \"ALE/Gravitar-v5\",\n",
    "            \"ALE/BattleZone-v5\",\n",
    "            \"ALE/Qbert-v5\",\n",
    "        ],\n",
    "        (\"config\", \"working_dir\"): \"baselines\",\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "    }\n",
    "\n",
    "    filtered_df = filter_df_by_criteria(atari_df, criteria)\n",
    "    metrics_keys2 = [\n",
    "        \"batch/end/SVD/effective_rank_vetterli/features_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/end/SVD/srank_kumar/features_policy_batch\",\n",
    "        \"batch/end/SVD/feature_rank_lyle/features_policy_batch\",\n",
    "        \"batch/end/SVD/pytorch_rank/features_policy_batch\",\n",
    "    ]\n",
    "    worst_correlations_score = worst_correlations(filtered_df, metrics_keys2)\n",
    "    plot_correlation_matrices(worst_correlations_score, metrics_keys2, \"worst_atari\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c963cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_worst_ranks_atari()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fdff55",
   "metadata": {},
   "source": [
    "# MuJoCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd4546",
   "metadata": {},
   "source": [
    "## Loading the runs from raw logs in a df and saving them combined .csv files\n",
    "### Training\n",
    "<div style=\"background-color: #F9F9F9; border-left: 5px solid #CC0000; padding: 10px; margin: 20px 0;\">\n",
    "    <strong> Skip to loading the combined raw logs .csv if you don't have the raw logs.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e15137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data is somewhere else create a symlink to it as your $PROJECT_DIR/outputs/rlconf\n",
    "# ln -s <path-to-rlconf-containing-data> $PROJECT_DIR/outputs/rlconf\n",
    "\n",
    "root_log_dir = f\"../outputs/rlconf/solve/mujoco-ppo/control/shared-trunk/2024-03-01_15-56-01-512787\"\n",
    "\n",
    "all_keys = set()\n",
    "log_subdirs = [\"logs/models\", \"logs/minibatch\", \"logs/eval\", \"logs/epoch\", \"logs/batch\"]\n",
    "n = len(os.listdir(root_log_dir))\n",
    "\n",
    "for subdir in log_subdirs:\n",
    "    subdir_path = os.path.join(root_log_dir, subdir)\n",
    "    if not os.path.exists(subdir_path):\n",
    "        print(f\"Subdirectory {subdir} does not exist.\")\n",
    "        continue\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        if file_path.endswith(\".tar\"):\n",
    "            loaded_data = torch.load(file_path, map_location=torch.device(\"cpu\"))\n",
    "            all_keys.update(loaded_data.keys())\n",
    "all_keys = list(all_keys)\n",
    "all_keys.sort()\n",
    "\n",
    "\n",
    "def get_nested_config_value(config, nested_key):\n",
    "    \"\"\"\n",
    "    Retrieve a value from a nested dictionary using a list of keys.\n",
    "    :param config: The configuration dictionary.\n",
    "    :param nested_key: A list of keys representing the path to the desired value.\n",
    "    :return: The value if found, None otherwise.\n",
    "    \"\"\"\n",
    "    for key in nested_key:\n",
    "        if isinstance(config, dict) and key in config:\n",
    "            config = config[key]\n",
    "        else:\n",
    "            return None\n",
    "    return config\n",
    "\n",
    "\n",
    "def config_matches_criteria(config, criteria):\n",
    "    \"\"\"\n",
    "    Check if the configuration matches the given criteria, supporting nested keys.\n",
    "    :param config: The configuration dictionary from the YAML file.\n",
    "    :param criteria: The criteria dictionary to match against, with nested keys as tuples.\n",
    "    :return: True if the config matches the criteria, False otherwise.\n",
    "    \"\"\"\n",
    "    for keys, value in criteria.items():\n",
    "        # Support for nested keys represented as tuples in criteria\n",
    "        nested_keys = keys if isinstance(keys, tuple) else (keys,)\n",
    "        config_value = get_nested_config_value(config, nested_keys)\n",
    "        if config_value != value:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def flatten_config(config, parent_key=\"\", sep=\"/\"):\n",
    "    items = []\n",
    "    for k, v in config.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_config(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                items.extend(flatten_config({f\"{new_key}{sep}{i}\": item}, \"\", sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def find_matching_configs(root_path, criteria):\n",
    "    \"\"\"\n",
    "    Find subfolders where the config matches the given criteria within each subfolder of root_path.\n",
    "    \"\"\"\n",
    "    matching_paths = []\n",
    "\n",
    "    # Iterate over each subfolder in the given root_path\n",
    "    for category in tqdm(os.listdir(root_path), total=len(os.listdir(root_path)), desc=\"Find matching path\"):\n",
    "        category_path = os.path.join(root_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue  # Skip if not a directory\n",
    "        if category == \"baselines\":\n",
    "            # Iterate over each run folder within the category\n",
    "            for run_folder in os.listdir(category_path):\n",
    "                config_path = os.path.join(category_path, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                # Check if the config_resolved.yaml file exists\n",
    "                if os.path.exists(config_path):\n",
    "                    with open(config_path, \"r\") as file:\n",
    "                        config = yaml.safe_load(file)\n",
    "\n",
    "                    # Check if the config matches the given criteria\n",
    "                    if config_matches_criteria(config, criteria):\n",
    "                        matching_paths.append(os.path.join(category_path, run_folder))\n",
    "        else:\n",
    "            for category2 in os.listdir(category_path):\n",
    "                category_path2 = os.path.join(category_path, category2)\n",
    "                for run_folder in os.listdir(category_path2):\n",
    "                    config_path = os.path.join(category_path2, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                    # Check if the config_resolved.yaml file exists\n",
    "                    if os.path.exists(config_path):\n",
    "                        with open(config_path, \"r\") as file:\n",
    "                            config = yaml.safe_load(file)\n",
    "\n",
    "                        # Check if the config matches the given criteria\n",
    "                        if config_matches_criteria(config, criteria):\n",
    "                            matching_paths.append(os.path.join(category_path2, run_folder))\n",
    "\n",
    "    return matching_paths\n",
    "\n",
    "\n",
    "def compile_data_debug(matching_paths, metric_keys):\n",
    "    data = []\n",
    "    nan_counter = 0  # Counter for NaN occurrences for the specific key\n",
    "\n",
    "    for path in tqdm(matching_paths, total=len(matching_paths), desc=\"Filling df\"):\n",
    "        # Load and flatten the config\n",
    "        config_path = os.path.join(path, \"config\", \"config_resolved.yaml\")\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        flat_config = flatten_config(config)\n",
    "        flat_config = {f\"config/{key}\": value for key, value in flat_config.items()}  # Prefix with \"config/\"\n",
    "\n",
    "        # Navigate to the logs/batch folder and read metrics\n",
    "        batch_folder = os.path.join(path, \"logs\", \"batch\")\n",
    "        if os.path.exists(batch_folder):\n",
    "            for log_file in os.listdir(batch_folder):\n",
    "                log_path = os.path.join(batch_folder, log_file)\n",
    "                if log_path.endswith(\".tar\"):\n",
    "                    loaded_data = torch.load(log_path, map_location=torch.device(\"cpu\"))\n",
    "                    # Initialize a record with the flattened config\n",
    "                    record = flat_config.copy()\n",
    "\n",
    "                    for metric_key in metric_keys:\n",
    "                        metric_value = loaded_data.get(metric_key, None)\n",
    "\n",
    "                        record[metric_key] = metric_value\n",
    "\n",
    "                    data.append(record)\n",
    "\n",
    "    # Create a DataFrame from the compiled data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "path_folder = \"../outputs/rlconf/solve/mujoco-ppo/\"\n",
    "# Define criteria with nested keys as tuples\n",
    "criteria = {}\n",
    "matching_paths = find_matching_configs(path_folder, criteria)\n",
    "\n",
    "mujoco_df = compile_data_debug(matching_paths, all_keys)\n",
    "print(mujoco_df.head())\n",
    "\n",
    "mujoco_df.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/mujoco.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mujoco_df.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/mujoco.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d05b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the time of submission to RLC\n",
    "# Should expect 624 runs\n",
    "# 240 baselines = 4 maps * 3 epochs * 2 activations * 2 lr schedules * 5 seeds\n",
    "# 384 interventions = 4 interventions * 2 maps * 3 epochs * 2 activations * (2 lr schedules * 3 seeds + 1 lr schedule * 2 seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9525520",
   "metadata": {},
   "source": [
    "### Plasticity\n",
    "<div style=\"background-color: #F9F9F9; border-left: 5px solid #CC0000; padding: 10px; margin: 20px 0;\">\n",
    "    <strong> Skip to loading the combined raw logs .csv if you don't have the raw logs.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_log_dir = \"../outputs/rlconf/capacity/mujoco-ppo/all/2024-04-08_12-52-56-917065/\"\n",
    "\n",
    "all_keys = set()\n",
    "log_subdirs = [\"logs/checkpoint\", \"logs/minibatch\", \"logs/model\", \"logs/epoch\"]\n",
    "n = len(os.listdir(root_log_dir))\n",
    "\n",
    "for subdir in log_subdirs:\n",
    "    subdir_path = os.path.join(root_log_dir, subdir)\n",
    "    if not os.path.exists(subdir_path):\n",
    "        print(f\"Subdirectory {subdir} does not exist.\")\n",
    "        continue\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        if file_path.endswith(\".tar\"):\n",
    "            loaded_data = torch.load(file_path, map_location=torch.device(\"cpu\"))\n",
    "            all_keys.update(loaded_data.keys())\n",
    "all_keys = list(all_keys)\n",
    "all_keys.sort()\n",
    "\n",
    "\n",
    "def get_nested_config_value(config, nested_key):\n",
    "    \"\"\"\n",
    "    Retrieve a value from a nested dictionary using a list of keys.\n",
    "    :param config: The configuration dictionary.\n",
    "    :param nested_key: A list of keys representing the path to the desired value.\n",
    "    :return: The value if found, None otherwise.\n",
    "    \"\"\"\n",
    "    for key in nested_key:\n",
    "        if isinstance(config, dict) and key in config:\n",
    "            config = config[key]\n",
    "        else:\n",
    "            return None\n",
    "    return config\n",
    "\n",
    "\n",
    "def config_matches_criteria(config, criteria):\n",
    "    \"\"\"\n",
    "    Check if the configuration matches the given criteria, supporting nested keys.\n",
    "    :param config: The configuration dictionary from the YAML file.\n",
    "    :param criteria: The criteria dictionary to match against, with nested keys as tuples.\n",
    "    :return: True if the config matches the criteria, False otherwise.\n",
    "    \"\"\"\n",
    "    for keys, value in criteria.items():\n",
    "        # Support for nested keys represented as tuples in criteria\n",
    "        nested_keys = keys if isinstance(keys, tuple) else (keys,)\n",
    "        config_value = get_nested_config_value(config, nested_keys)\n",
    "        if config_value != value:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def flatten_config(config, parent_key=\"\", sep=\"/\"):\n",
    "    items = []\n",
    "    for k, v in config.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_config(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                items.extend(flatten_config({f\"{new_key}{sep}{i}\": item}, \"\", sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def find_matching_configs(root_path, criteria):\n",
    "    \"\"\"\n",
    "    Find subfolders where the config matches the given criteria within each subfolder of root_path.\n",
    "    \"\"\"\n",
    "    matching_paths = []\n",
    "\n",
    "    # Iterate over each subfolder in the given root_path\n",
    "    for category in tqdm(os.listdir(root_path), total=len(os.listdir(root_path)), desc=\"Find matching path\"):\n",
    "        category_path = os.path.join(root_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue  # Skip if not a directory\n",
    "        if category == \"all\":\n",
    "            # Iterate over each run folder within the category\n",
    "            for run_folder in os.listdir(category_path):\n",
    "                config_path = os.path.join(category_path, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                # Check if the config_resolved.yaml file exists\n",
    "                if os.path.exists(config_path):\n",
    "                    with open(config_path, \"r\") as file:\n",
    "                        config = yaml.safe_load(file)\n",
    "\n",
    "                    # Check if the config matches the given criteria\n",
    "                    if config_matches_criteria(config, criteria):\n",
    "                        matching_paths.append(os.path.join(category_path, run_folder))\n",
    "        else:\n",
    "            for category2 in os.listdir(category_path):\n",
    "                category_path2 = os.path.join(category_path, category2)\n",
    "                for run_folder in os.listdir(category_path2):\n",
    "                    config_path = os.path.join(category_path2, run_folder, \"config\", \"config_resolved.yaml\")\n",
    "\n",
    "                    # Check if the config_resolved.yaml file exists\n",
    "                    if os.path.exists(config_path):\n",
    "                        with open(config_path, \"r\") as file:\n",
    "                            config = yaml.safe_load(file)\n",
    "\n",
    "                        # Check if the config matches the given criteria\n",
    "                        if config_matches_criteria(config, criteria):\n",
    "                            matching_paths.append(os.path.join(category_path2, run_folder))\n",
    "\n",
    "    return matching_paths\n",
    "\n",
    "\n",
    "def compile_data_debug(matching_paths, metric_keys):\n",
    "    data = []\n",
    "    nan_counter = 0  # Counter for NaN occurrences for the specific key\n",
    "\n",
    "    for path in tqdm(matching_paths, total=len(matching_paths), desc=\"Filling df\"):\n",
    "        # Load and flatten the config\n",
    "        config_path = os.path.join(path, \"config\", \"config_resolved.yaml\")\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        flat_config = flatten_config(config)\n",
    "        flat_config = {f\"config/{key}\": value for key, value in flat_config.items()}  # Prefix with \"config/\"\n",
    "\n",
    "        # Navigate to the logs/batch folder and read metrics\n",
    "        batch_folder = os.path.join(path, \"logs\", \"checkpoint\")\n",
    "        if os.path.exists(batch_folder):\n",
    "            for log_file in os.listdir(batch_folder):\n",
    "                log_path = os.path.join(batch_folder, log_file)\n",
    "                if log_path.endswith(\".tar\"):\n",
    "                    loaded_data = torch.load(log_path, map_location=torch.device(\"cpu\"))\n",
    "                    # Initialize a record with the flattened config\n",
    "                    record = flat_config.copy()\n",
    "\n",
    "                    for metric_key in metric_keys:\n",
    "                        metric_value = loaded_data.get(metric_key, None)\n",
    "\n",
    "                        record[metric_key] = metric_value\n",
    "\n",
    "                    data.append(record)\n",
    "\n",
    "    # Create a DataFrame from the compiled data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "path_folder = \"../outputs/rlconf/capacity/mujoco-ppo/\"\n",
    "# Define criteria with nested keys as tuples\n",
    "criteria = {}\n",
    "matching_paths = find_matching_configs(path_folder, criteria)\n",
    "\n",
    "mujoco_df_capacity = compile_data_debug(matching_paths, all_keys)\n",
    "print(atari_df_capacity.head())\n",
    "mujoco_df_capacity.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/mujoco-capacity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mujoco_df_capacity.to_csv(\"../outputs/rlconf-plotting/combined-raw-logs/mujoco-capacity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the time of submission to RLC\n",
    "# Should expect 624 runs\n",
    "# 240 baselines = 4 maps * 3 epochs * 2 activations * 2 lr schedules * 5 seeds\n",
    "# 384 interventions = 4 interventions * 2 maps * 3 epochs * 2 activations * (2 lr schedules * 3 seeds + 1 lr schedule * 2 seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803d28d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #F9F9F9; border-left: 5px solid #CC0000; padding: 10px; margin: 20px 0;\">\n",
    "    <strong> Load the combined raw logs .csv files here. If you did not generate them you can find the instructions to download them in $PROJECT_ROOT/outputs/README.md. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco_df = pd.read_csv(\"../outputs/rlconf-plotting/combined-raw-logs/mujoco.csv\")\n",
    "mujoco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ed902",
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco_df_capacity = pd.read_csv(\"../outputs/rlconf-plotting/combined-raw-logs/mujoco-capacity.csv\")\n",
    "mujoco_df_capacity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561ecd3",
   "metadata": {},
   "source": [
    "# Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure1_mujoco(name, activation):\n",
    "    group_by_cols = [\"config/optim/num_epochs\"]\n",
    "\n",
    "    metrics_keys = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_value_batch\",\n",
    "    ]\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"optim\", \"reset_state\"): [False],\n",
    "        (\"config\", \"models\", \"share_features\"): [False],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0.0],\n",
    "        (\"config\", \"working_dir\"): \"baselines\",\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "    }\n",
    "    fig1_df = filter_df_by_criteria(mujoco_df, criteria)\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"optim\", \"reset_state\"): [False],\n",
    "        (\"config\", \"models\", \"share_features\"): [False],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0.0],\n",
    "        (\"config\", \"solve_dir\"): \"baselines\",\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "    }\n",
    "    fig1_df_plasticity = filter_df_by_criteria(mujoco_df_capacity, criteria)\n",
    "    f_name = f\"1-{name}-{activation}\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    plot_shaded_metrics_side_by_side_plasticity_smooth(\n",
    "        fig1_df,\n",
    "        fig1_df_plasticity,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys,\n",
    "        [\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/value\",\n",
    "        ],\n",
    "        [\n",
    "            \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "        ],\n",
    "        [\n",
    "            \"Episode return\",\n",
    "            \"Feature rank policy (PCA)\",\n",
    "            \"Norm preactivation policy\",\n",
    "            \"Feature rank critic (PCA)\",\n",
    "            \"Plasticity loss policy\",\n",
    "            \"Plasticity loss critic\",\n",
    "        ],\n",
    "        title=name,\n",
    "        name_=f_name,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cdcb95",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_mujoco(\"Ant-v4\", \"Tanh\")\n",
    "figure1_mujoco(\"Hopper-v4\", \"Tanh\")\n",
    "figure1_mujoco(\"Humanoid-v4\", \"Tanh\")\n",
    "figure1_mujoco(\"HalfCheetah-v4\", \"Tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb77ca5",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f09c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_mujoco(\"Ant-v4\", \"ReLU\")\n",
    "figure1_mujoco(\"Hopper-v4\", \"ReLU\")\n",
    "figure1_mujoco(\"Humanoid-v4\", \"ReLU\")\n",
    "figure1_mujoco(\"HalfCheetah-v4\", \"ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467564ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot in figure 1\n",
    "def plot_shaded_metrics_side_by_side_plasticity_smooth_mujoco(\n",
    "    df, df2, group_by_cols, x_col, metrics_keys, metrics_keys2, log_key=\"\", subplot_titles=None, title=\"\", name_=\"\"\n",
    "):\n",
    "    if name_.split(\"-\")[-1] == \"bis\":\n",
    "        color_cycler = cycler(color=[\"#D55E00\", \"#56B4E9\", \"#0072B2\", \"#009E73\", \"#E69F00\"])\n",
    "        plt.rc(\"axes\", prop_cycle=color_cycler)\n",
    "    else:\n",
    "        plt.rc(\"axes\", prop_cycle=line_cycler)\n",
    "    if isinstance(group_by_cols, list) and len(group_by_cols) == 1:\n",
    "        group_by = group_by_cols[0]\n",
    "    else:\n",
    "        group_by = group_by_cols\n",
    "\n",
    "    grouped = df.groupby(group_by)\n",
    "    num_metrics = len(metrics_keys) + 2\n",
    "    alpha = 0.05\n",
    "    fig, axs = plt.subplots(1, num_metrics, figsize=(40, 8), sharex=True)\n",
    "    special_keys = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_value_batch\",\n",
    "    ]\n",
    "    for metric_idx, metric_key in enumerate(metrics_keys):\n",
    "        for name, group in grouped:\n",
    "            if metric_key in special_keys:\n",
    "                # Calculate EMA for special keys\n",
    "                ema_mean = group.groupby(x_col)[metric_key].mean().ewm(alpha=alpha, adjust=False).mean()\n",
    "                ema_min = group.groupby(x_col)[metric_key].min().ewm(alpha=alpha, adjust=False).mean()\n",
    "                ema_max = group.groupby(x_col)[metric_key].max().ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "                if metric_idx == 3:\n",
    "                    axs[metric_idx + 1].plot(\n",
    "                        ema_mean.index, ema_mean, label=name if metric_idx == 0 else \"\", linewidth=4.0\n",
    "                    )\n",
    "                    axs[metric_idx + 1].fill_between(ema_mean.index, ema_min, ema_max, alpha=0.3)\n",
    "                else:\n",
    "                    axs[metric_idx].plot(ema_mean.index, ema_mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                    axs[metric_idx].fill_between(ema_mean.index, ema_min, ema_max, alpha=0.3)\n",
    "            else:\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(x_col)[metric_key].mean()\n",
    "                min_val = group.groupby(x_col)[metric_key].min()\n",
    "                max_val = group.groupby(x_col)[metric_key].max()\n",
    "                if metric_idx == 3:\n",
    "                    axs[metric_idx + 1].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                    axs[metric_idx + 1].fill_between(\n",
    "                        mean.index, min_val, max_val, alpha=0.3\n",
    "                    )  # Use min and max for shaded area\n",
    "                else:\n",
    "                    axs[metric_idx].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                    axs[metric_idx].fill_between(\n",
    "                        mean.index, min_val, max_val, alpha=0.3\n",
    "                    )  # Use min and max for shaded area\n",
    "        if metric_idx == 3:\n",
    "            axs[metric_idx + 1].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[metric_idx + 1].set_ylabel(subplot_titles[metric_idx], fontsize=30)\n",
    "            axs[metric_idx + 1].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[metric_idx + 1].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[metric_idx + 1].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[metric_idx + 1].yaxis.get_offset_text().set_fontsize(25)\n",
    "        else:\n",
    "            axs[metric_idx].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[metric_idx].set_ylabel(subplot_titles[metric_idx], fontsize=30)\n",
    "            axs[metric_idx].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[metric_idx].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[metric_idx].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[metric_idx].yaxis.get_offset_text().set_fontsize(25)\n",
    "\n",
    "        if metric_key in log_key:\n",
    "            axs[metric_idx].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "\n",
    "        axs[metric_idx].grid(linestyle=\"dotted\")\n",
    "\n",
    "    for metric_idx2, metric_key2 in enumerate(metrics_keys2):\n",
    "        if metric_idx2 == 0:\n",
    "            grouped = df2.groupby(group_by)\n",
    "            for name, group in grouped:\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(\"capacity-counters/env_steps\")[metric_key2].mean()\n",
    "                min_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].min()\n",
    "                max_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].max()\n",
    "\n",
    "                axs[3].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                axs[3].fill_between(mean.index, min_val, max_val, alpha=0.3)  # Use min and max for shaded area\n",
    "            axs[len(metrics_keys) - 1].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[len(metrics_keys) - 1].set_ylabel(subplot_titles[len(metrics_keys)], fontsize=30)\n",
    "            axs[len(metrics_keys) - 1].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[len(metrics_keys) - 1].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[len(metrics_keys) - 1].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[len(metrics_keys) - 1].yaxis.get_offset_text().set_fontsize(25)\n",
    "            if metric_key2 in log_key:\n",
    "                axs[len(metrics_keys) - 1].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "        else:\n",
    "            grouped = df2.groupby(group_by)\n",
    "            for name, group in grouped:\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(\"capacity-counters/env_steps\")[metric_key2].mean()\n",
    "                min_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].min()\n",
    "                max_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].max()\n",
    "\n",
    "                axs[len(metrics_keys) + 1].plot(mean.index, mean, label=name if metric_idx == 0 else \"\", linewidth=4.0)\n",
    "                axs[len(metrics_keys) + 1].fill_between(\n",
    "                    mean.index, min_val, max_val, alpha=0.3\n",
    "                )  # Use min and max for shaded area\n",
    "            axs[len(metrics_keys) + 1].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[len(metrics_keys) + 1].set_ylabel(subplot_titles[len(metrics_keys) + 1], fontsize=30)\n",
    "            axs[len(metrics_keys) + 1].tick_params(axis=\"x\", labelsize=30)\n",
    "            axs[len(metrics_keys) + 1].tick_params(axis=\"y\", labelsize=30)\n",
    "            axs[len(metrics_keys) + 1].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[len(metrics_keys) + 1].yaxis.get_offset_text().set_fontsize(25)\n",
    "            if metric_key2 in log_key:\n",
    "                axs[len(metrics_keys) + 1].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "        axs[len(metrics_keys) + metric_idx2].grid(linestyle=\"dotted\")\n",
    "    # fig.suptitle(title, fontsize=30)\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    print(labels)\n",
    "    # labels = [label + \" epochs\" for label in labels]\n",
    "    lab = []\n",
    "    for label in labels:\n",
    "        if label.startswith(\"(\") and label.endswith(\")\"):\n",
    "            axs[0].text(\n",
    "                0.02,\n",
    "                0.90,\n",
    "                f\"{title}\\n{df['config/optim/num_epochs'].values[0]} epochs\",\n",
    "                fontsize=20,\n",
    "                fontweight=\"bold\",\n",
    "                transform=axs[0].transAxes,\n",
    "            )\n",
    "            label = label[1:-1]\n",
    "            parts = label.split(\",\")\n",
    "            # Assign and convert each part to the correct type\n",
    "            share = parts[0]\n",
    "            trust = float(parts[1])\n",
    "            reset = parts[2]\n",
    "            all_lay = parts[3]\n",
    "            if share == \"True\":\n",
    "                lab.append(f\"Share actor and critic features\")\n",
    "            else:\n",
    "                if reset == \" True\":\n",
    "                    lab.append(\"Reset Adam\")\n",
    "                else:\n",
    "                    if trust == 1:\n",
    "                        if all_lay == \"False\":\n",
    "                            lab.append(f\"Regularize last preactivation\")\n",
    "                        else:\n",
    "                            lab.append(f\"Regularize all preactivations\")\n",
    "                    else:\n",
    "                        lab.append(\"No intervention\")\n",
    "        else:\n",
    "            lab.append(label + \" epochs\")\n",
    "            axs[0].text(0.02, 0.95, f\"{title}\", fontsize=20, fontweight=\"bold\", transform=axs[0].transAxes)\n",
    "    legend_properties = {\"weight\": \"bold\"}\n",
    "\n",
    "    if len(lab) == 4:\n",
    "        box_to_anchor = (0.25, 1.05)\n",
    "\n",
    "    elif len(lab) == 3:\n",
    "        box_to_anchor = (0.4, 1.05)\n",
    "    else:\n",
    "        box_to_anchor = (0.15, 1.05)\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        lab,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=box_to_anchor,\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=30,\n",
    "        ncol=len(lab),\n",
    "        frameon=False,\n",
    "        handlelength=1,\n",
    "        handletextpad=0.5,\n",
    "        columnspacing=1,\n",
    "    )\n",
    "    plt.subplots_adjust(right=1, top=0.90)  # Adjust subplot params to make room for the legend\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/Figure-1-{name_}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def figure1_mujoco_bis(name, num_epochs, activation):\n",
    "    group_by_cols = [\n",
    "        \"config/models/share_features\",\n",
    "        \"config/loss/policy/kwargs/feature_trust_region_coef\",\n",
    "        \"config/optim/reset_state\",\n",
    "        \"config/loss/policy/kwargs/feature_trust_all_layers\",\n",
    "    ]\n",
    "\n",
    "    metrics_keys = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_value_batch\",\n",
    "    ]\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [num_epochs],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"algo\"): [\"ppo-clip\"],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0, 1, 10],\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "        (\"config\", \"working_dir\"): [\n",
    "            \"baseline\",\n",
    "            \"experiment\",\n",
    "            \"optimizer\",\n",
    "            \"regularize\",\n",
    "            \"regularize-all-layers\",\n",
    "            \"shared-trunk\",\n",
    "        ],\n",
    "    }\n",
    "    fig1_df = filter_df_by_criteria(mujoco_df, criteria)\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [num_epochs],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"algo\"): [\"ppo-clip\"],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0, 1, 10],\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "        (\"config\", \"solve_dir\"): [\n",
    "            \"baseline\",\n",
    "            \"experiment\",\n",
    "            \"optimizer\",\n",
    "            \"regularize\",\n",
    "            \"regularize-all-layers\",\n",
    "            \"shared-trunk\",\n",
    "        ],\n",
    "    }\n",
    "    fig1_df_plasticity = filter_df_by_criteria(mujoco_df_capacity, criteria)\n",
    "    f_name = f\"2-{name}-{num_epochs}-{activation}-epochs-bis\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    plot_shaded_metrics_side_by_side_plasticity_smooth_mujoco(\n",
    "        fig1_df,\n",
    "        fig1_df_plasticity,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys,\n",
    "        [\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/value\",\n",
    "        ],\n",
    "        \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        [\n",
    "            \"Episode return\",\n",
    "            \"Feature rank policy (PCA)\",\n",
    "            \"Norm preactivation policy\",\n",
    "            \"Feature rank critic (PCA)\",\n",
    "            \"Plasticity loss policy\",\n",
    "            \"Plasticity loss critic\",\n",
    "        ],\n",
    "        title=name,\n",
    "        name_=f_name,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_mujoco_bis(\"Hopper-v4\", 10, \"Tanh\")\n",
    "figure1_mujoco_bis(\"Hopper-v4\", 15, \"Tanh\")\n",
    "figure1_mujoco_bis(\"Hopper-v4\", 20, \"Tanh\")\n",
    "figure1_mujoco_bis(\"Humanoid-v4\", 10, \"Tanh\")\n",
    "figure1_mujoco_bis(\"Humanoid-v4\", 15, \"Tanh\")\n",
    "figure1_mujoco_bis(\"Humanoid-v4\", 20, \"Tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae175a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_mujoco_bis(\"Hopper-v4\", 10, \"ReLU\")\n",
    "figure1_mujoco_bis(\"Hopper-v4\", 15, \"ReLU\")\n",
    "figure1_mujoco_bis(\"Hopper-v4\", 20, \"ReLU\")\n",
    "figure1_mujoco_bis(\"Humanoid-v4\", 10, \"ReLU\")\n",
    "figure1_mujoco_bis(\"Humanoid-v4\", 15, \"ReLU\")\n",
    "figure1_mujoco_bis(\"Humanoid-v4\", 20, \"ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d135b3",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d28c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure2_mujoco(name, activation):\n",
    "    metrics_keys2 = [\n",
    "        \"batch/first_epoch/first_minibatch/loss/entropy\",\n",
    "        \"batch/start/action_diversity/policy_variance\",\n",
    "        \"batch/start/dead_neurons/features_policy_batch\",\n",
    "    ]\n",
    "    y_axis = [\"Entropy\", \"Policy variance\", \"Dead neurons policy\"]\n",
    "    f_name = f\"1-{name}-{activation}\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "        (\"config\", \"optim\", \"reset_state\"): [False],\n",
    "        (\"config\", \"models\", \"share_features\"): [False],\n",
    "        (\"config\", \"loss\", \"policy\", \"kwargs\", \"feature_trust_region_coef\"): [0.0],\n",
    "        (\"config\", \"working_dir\"): \"baselines\",\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "    }\n",
    "    fig2_df = filter_df_by_criteria(mujoco_df, criteria)\n",
    "    group_by_cols = [\"config/optim/num_epochs\"]\n",
    "    plot_shaded_metrics_side_by_side_smooth(\n",
    "        fig2_df,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys2,\n",
    "        \"batch/start/action_diversity/policy_variance\",\n",
    "        y_axis,\n",
    "        title=name,\n",
    "        name_=f_name,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0565bf4",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2da702",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure2_mujoco(\"HalfCheetah-v4\", \"ReLU\")\n",
    "figure2_mujoco(\"Humanoid-v4\", \"ReLU\")\n",
    "figure2_mujoco(\"Ant-v4\", \"ReLU\")\n",
    "figure2_mujoco(\"Hopper-v4\", \"ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cc2a6",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ecad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure2_mujoco(\"HalfCheetah-v4\", \"Tanh\")\n",
    "figure2_mujoco(\"Humanoid-v4\", \"Tanh\")\n",
    "figure2_mujoco(\"Ant-v4\", \"Tanh\")\n",
    "figure2_mujoco(\"Hopper-v4\", \"Tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a6188",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shaded_metrics_side_by_side_plasticity_fig3(\n",
    "    df, df2, group_by_cols, x_col, metrics_keys, metrics_keys2, log_key=\"\", subplot_titles=None, title=\"\", name_=\"\"\n",
    "):\n",
    "    plt.rc(\"axes\", prop_cycle=line_cycler)\n",
    "    if isinstance(group_by_cols, list) and len(group_by_cols) == 1:\n",
    "        group_by = group_by_cols[0]\n",
    "    else:\n",
    "        group_by = group_by_cols\n",
    "\n",
    "    grouped = df.groupby(group_by)\n",
    "    num_metrics = len(metrics_keys) + 1\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_metrics, figsize=(40, 8), sharex=True)\n",
    "    alpha = 0.05\n",
    "    positions = {\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\": 0,\n",
    "        \"batch/perf/avg_return_raw\": 2,\n",
    "        \"batch/diff/avg_prob_ratio_below_epsilon\": 3,\n",
    "        \"batch/end/action_diversity/policy_variance\": 4,\n",
    "        \"batch/last_epoch/last_minibatch/loss/loss_policy\": 5,\n",
    "        \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\": 1,\n",
    "    }\n",
    "    legend_info = {}  # Dictionary to track unique legends\n",
    "    for metric_idx, metric_key in enumerate(metrics_keys):\n",
    "        for name, group in grouped:\n",
    "            if metric_key in [\n",
    "                \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "                \"batch/perf/avg_return_raw\",\n",
    "                \"batch/diff/avg_prob_ratio_below_epsilon\",\n",
    "                \"batch/end/action_diversity/policy_variance\",\n",
    "                \"batch/last_epoch/last_minibatch/loss/loss_policy\",\n",
    "            ]:\n",
    "                env_name = group[\"config/env/name\"].iloc[0]  # Assuming uniform configuration within the group\n",
    "                num_epochs = group[\"config/optim/num_epochs\"].iloc[0]\n",
    "                activation = group[\"config/models/activation\"].iloc[0]\n",
    "                seed = group[\"config/seed\"].iloc[0]\n",
    "                label = f\"{env_name}, {num_epochs} epochs, seed={seed}\"\n",
    "\n",
    "                group = group.dropna(subset=[metric_key])\n",
    "                if metric_key == \"batch/last_epoch/last_minibatch/loss/loss_policy\":\n",
    "                    ema_mean = -group.groupby(x_col)[metric_key].mean().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_min = -group.groupby(x_col)[metric_key].min().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_max = -group.groupby(x_col)[metric_key].max().ewm(alpha=alpha, adjust=False).mean()\n",
    "                else:\n",
    "                    ema_mean = group.groupby(x_col)[metric_key].mean().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_min = group.groupby(x_col)[metric_key].min().ewm(alpha=alpha, adjust=False).mean()\n",
    "                    ema_max = group.groupby(x_col)[metric_key].max().ewm(alpha=alpha, adjust=False).mean()\n",
    "                (line,) = axs[positions[metric_key]].plot(ema_mean.index, ema_mean, label=label, linewidth=4.0)\n",
    "                axs[positions[metric_key]].fill_between(ema_mean.index, ema_min, ema_max, alpha=0.3)\n",
    "                axs[positions[metric_key]].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "                axs[positions[metric_key]].set_ylabel(subplot_titles[positions[metric_key]], fontsize=25)\n",
    "                axs[positions[metric_key]].tick_params(axis=\"x\", labelsize=25)\n",
    "                axs[positions[metric_key]].tick_params(axis=\"y\", labelsize=25)\n",
    "                axs[positions[metric_key]].xaxis.get_offset_text().set_fontsize(25)\n",
    "                axs[positions[metric_key]].yaxis.get_offset_text().set_fontsize(25)\n",
    "                if label not in legend_info:\n",
    "                    legend_info[label] = line\n",
    "\n",
    "        if metric_key in log_key:\n",
    "            axs[positions[metric_key]].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "        if metric_key == \"batch/last_epoch/last_minibatch/loss/loss_policy\":\n",
    "            axs[positions[metric_key]].set_ylim(bottom=-0.1)\n",
    "            axs[positions[metric_key]].set_ylim(top=0.1)\n",
    "\n",
    "        axs[positions[metric_key]].grid(linestyle=\"dotted\")\n",
    "\n",
    "    for metric_idx2, metric_key2 in enumerate(metrics_keys2):\n",
    "        if metric_idx2 == 0:\n",
    "            grouped = df2.groupby(group_by)\n",
    "            for name, group in grouped:\n",
    "                env_name = group[\"config/env/name\"].iloc[0]  # Assuming uniform configuration within the group\n",
    "                num_epochs = group[\"config/optim/num_epochs\"].iloc[0]\n",
    "                activation = group[\"config/models/activation\"].iloc[0]\n",
    "                label = f\"{env_name}, {num_epochs} epochs, {activation}\"\n",
    "                # Here, adjust the groupby for mean, min, and max to directly use the column name\n",
    "                mean = group.groupby(\"capacity-counters/env_steps\")[metric_key2].mean()\n",
    "                min_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].min()\n",
    "                max_val = group.groupby(\"capacity-counters/env_steps\")[metric_key2].max()\n",
    "\n",
    "                (line,) = axs[positions[metric_key2]].plot(mean.index, mean, label=label, linewidth=4.0)\n",
    "                axs[positions[metric_key2]].fill_between(\n",
    "                    mean.index, min_val, max_val, alpha=0.3\n",
    "                )  # Use min and max for shaded area\n",
    "            axs[positions[metric_key2]].set_xlabel(\"Environment steps\", fontsize=25)\n",
    "            axs[positions[metric_key2]].set_ylabel(subplot_titles[positions[metric_key2]], fontsize=25)\n",
    "            axs[positions[metric_key2]].tick_params(axis=\"x\", labelsize=25)\n",
    "            axs[positions[metric_key2]].tick_params(axis=\"y\", labelsize=25)\n",
    "            axs[positions[metric_key2]].xaxis.get_offset_text().set_fontsize(25)\n",
    "            axs[positions[metric_key2]].yaxis.get_offset_text().set_fontsize(25)\n",
    "        if metric_key2 in log_key:\n",
    "            axs[positions[metric_key2]].set_yscale(\"log\")  # Apply log scale for the specific subplot\n",
    "\n",
    "        axs[positions[metric_key2]].grid(linestyle=\"dotted\")\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    labels = [label + \" epochs\" for label in labels]\n",
    "    # fig.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.95, 1.00), borderaxespad=0.0,fontsize=20)\n",
    "    # plt.subplots_adjust(right=1,top=0.8)  # Adjust subplot params to make room for the legend\n",
    "    color_legend = [\n",
    "        mlines.Line2D([], [], color=\"#E69F00\", linestyle=\"solid\", markersize=20, label=\"4 epochs\", linewidth=4),\n",
    "        mlines.Line2D([], [], color=\"#56B4E9\", linestyle=\"solid\", markersize=20, label=\"6 epochs\", linewidth=4),\n",
    "        mlines.Line2D([], [], color=\"#009E73\", linestyle=\"solid\", markersize=20, label=\"8 epochs\", linewidth=4),\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=list(legend_info.values()),\n",
    "        labels=list(legend_info.keys()),\n",
    "        loc=\"upper center\",  # This anchors the center of the legend at the provided coordinate\n",
    "        bbox_to_anchor=(-2.8, 1.2),  # Anchors the legend above the plot\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=25,\n",
    "        ncol=4,\n",
    "        frameon=False,\n",
    "        handlelength=1,\n",
    "        handletextpad=0.5,\n",
    "        columnspacing=1,\n",
    "    )\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/Figure-3-{name_}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure3_mujoco():\n",
    "    criteria = {\n",
    "        (\"config\", \"working_dir\"): [\n",
    "            \"baselines/2024-02-27_19-45-33-114578\",\n",
    "            \"baselines/2024-02-27_19-44-56-470648\",\n",
    "            \"baselines/2024-02-27_19-44-15-056169\",\n",
    "            \"baselines/2024-02-27_09-15-25-975421\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    one_game_5_df = filter_df_by_criteria(mujoco_df, criteria)\n",
    "\n",
    "    criteria = {\n",
    "        (\"config\", \"solve_dir\"): [\n",
    "            \"baselines/2024-02-27_19-45-33-114578\",\n",
    "            \"baselines/2024-02-27_19-44-56-470648\",\n",
    "            \"baselines/2024-02-27_19-44-15-056169\",\n",
    "            \"baselines/2024-02-27_09-15-25-975421\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    one_game_5_df2 = filter_df_by_criteria(mujoco_df_capacity, criteria)\n",
    "\n",
    "    group_by_cols = [\"config/working_dir\"]\n",
    "\n",
    "    metrics_keys2 = [\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"batch/diff/avg_prob_ratio_below_epsilon\",\n",
    "        \"batch/end/action_diversity/policy_variance\",\n",
    "        \"batch/last_epoch/last_minibatch/loss/loss_policy\",\n",
    "    ]\n",
    "\n",
    "    plot_shaded_metrics_side_by_side_plasticity_fig3(\n",
    "        one_game_5_df,\n",
    "        one_game_5_df2,\n",
    "        group_by_cols,\n",
    "        \"global_step\",\n",
    "        metrics_keys2,\n",
    "        [\"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\"],\n",
    "        [\n",
    "            \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "            \"batch/end/action_diversity/policy_variance\",\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "        ],\n",
    "        [\n",
    "            \"Rank policy (PCA)\",\n",
    "            \"Plasticity loss policy\",\n",
    "            \"Episode return\",\n",
    "            r\"Avg of prob ratios < 1 - $\\epsilon$\",\n",
    "            \"Policy variance\",\n",
    "            \"PPO-Clip objective\",\n",
    "        ],\n",
    "        title=\"\",\n",
    "        name_=\"5-mujoco\",\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure3_mujoco()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574ed73",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure5_mujoco(name, activation):\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): name,\n",
    "        (\"config\", \"working_dir\"): [\"baselines\"],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): False,\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [10, 15, 20],\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "    }\n",
    "    df_games = filter_df_by_criteria(mujoco_df, criteria)\n",
    "\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): name,\n",
    "        (\"config\", \"solve_dir\"): [\"baselines\"],\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): False,\n",
    "        (\"config\", \"optim\", \"num_epochs\"): [10, 15, 20],\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "    }\n",
    "    df_games_2 = filter_df_by_criteria(mujoco_df_capacity, criteria)\n",
    "\n",
    "    plot_correlation(\n",
    "        df_games,\n",
    "        df_games_2,\n",
    "        \"batch/diff/avg_prob_ratio_below_epsilon\",\n",
    "        \"batch/diff/min_prob_ratio_below_epsilon\",\n",
    "        \"\",\n",
    "        [\n",
    "            \"batch/start/dead_neurons/features_policy_batch\",\n",
    "            \"batch/start/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "            \"batch/start/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        ],\n",
    "        r\"Avg of prob ratios < 1 - $\\epsilon$\",\n",
    "        [\"Dead neurons policy\", \"Feature rank policy (PCA)\", \"Feature preactivation norm\"],\n",
    "        log=0,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7f9cb",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece01fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure5_mujoco([\"Ant-v4\"], \"ReLU\")\n",
    "figure5_mujoco([\"Humanoid-v4\"], \"ReLU\")\n",
    "figure5_mujoco([\"Hopper-v4\"], \"ReLU\")\n",
    "figure5_mujoco([\"HalfCheetah-v4\"], \"ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65662bbe",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure5_mujoco([\"Ant-v4\"], \"Tanh\")\n",
    "figure5_mujoco([\"Humanoid-v4\"], \"Tanh\")\n",
    "figure5_mujoco([\"Hopper-v4\"], \"Tanh\")\n",
    "figure5_mujoco([\"HalfCheetah-v4\"], \"Tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734fe7b9",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = [\n",
    "    \"Share actor and critic features\",\n",
    "    \"Reset Adam\",\n",
    "    \"Regularize all preactivations\",\n",
    "    \"Regularize last preactivation\",\n",
    "    \"No invervention\",\n",
    "]\n",
    "\n",
    "\n",
    "def plot_metrics_by_keywords_mujoco(df, df2, metrics, metric2, metric_name, keywords, steps_window=20000, save_name=\"\"):\n",
    "    \"\"\"\n",
    "    Plot box plots for given metrics, aggregating data by keywords found in 'config/working_dir'.\n",
    "\n",
    "    :param df: DataFrame containing the dataset.\n",
    "    :param metrics: List of metric keys to plot.\n",
    "    :param keywords: List of keywords to filter configurations by their working directory.\n",
    "    :param steps_window: Look at data within this window from the max global_step for averaging.\n",
    "    \"\"\"\n",
    "    # Prepare the aggregated data storage\n",
    "    keys = [\n",
    "        \"baselines\",\n",
    "        \"ppo-kl\",\n",
    "        \"ppo-early-stop\",\n",
    "        \"all\",\n",
    "        \"optimizer\",\n",
    "        \"regularize\",\n",
    "        \"regularize-all-layers\",\n",
    "        \"shared-trunk\",\n",
    "    ]\n",
    "    all_metrics = metrics + [metric2]\n",
    "    aggregated_data = {keyword: {metric: [] for metric in all_metrics} for keyword in keywords}\n",
    "\n",
    "    config_keys = [key for key in df.columns if key.startswith(\"config/\")]\n",
    "    unique_configs = df[config_keys].drop_duplicates()\n",
    "    colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]\n",
    "    return_nb = []\n",
    "    N_nb = []\n",
    "    return_steps = []\n",
    "    ratio_nb = []\n",
    "    ratio_N_nb = []\n",
    "    ratio_steps = []\n",
    "    return_info = []\n",
    "    ratio_info = []\n",
    "    ratio_res = []\n",
    "    return_res = []\n",
    "    for _, config_row in unique_configs.iterrows():\n",
    "        mask = create_mask_for_config(df, config_row, config_keys)\n",
    "        filtered_df = df.loc[mask].copy()\n",
    "        for keyword in keywords:\n",
    "            if \"algo\" in config_row[\"config/working_dir\"]:\n",
    "                continue\n",
    "            else:\n",
    "                if keyword in config_row[\"config/working_dir\"]:\n",
    "                    if config_row[\"config/loss/policy/kwargs/feature_trust_all_layers\"] == True:\n",
    "                        keyword_ = \"regularize-all-layers\"\n",
    "                    else:\n",
    "                        keyword_ = keyword\n",
    "                    for metric in metrics:\n",
    "                        if metric == \"ProbRatio\":\n",
    "                            metric_key1 = \"batch/diff/avg_prob_ratio_above_epsilon\"\n",
    "                            metric_key2 = \"batch/diff/avg_prob_ratio_below_epsilon\"\n",
    "\n",
    "                            # Remove rows where either metric_key1 or metric_key2 is NaN\n",
    "                            # filtered_df[metric_key1].replace([np.inf,float('inf')], 100, inplace=True)\n",
    "                            # filtered_df[metric_key2].replace([0], 0.01, inplace=True)\n",
    "                            df_no_nan = filtered_df.dropna(subset=[metric_key1, metric_key2])\n",
    "                            df_sorted = df_no_nan.sort_values(by=\"global_step\", ascending=True)  # Sort by 'global_step'\n",
    "                            B_max = df_sorted.index.max()\n",
    "                            # Get max 'global_step' before and after dropping NaNs for verification\n",
    "                            unfiltered_max = filtered_df[\"global_step\"].max()\n",
    "                            filtered_max = df_sorted[\"global_step\"].max()\n",
    "\n",
    "                            df_steps_max = df_no_nan.sort_values(by=\"global_step\", ascending=False)[\n",
    "                                \"global_step\"\n",
    "                            ].values.tolist()\n",
    "                            idx_max = 0\n",
    "\n",
    "                            five_percent_of_unfiltered_max = int(unfiltered_max * 0.05)\n",
    "\n",
    "                            # Initial threshold calculation\n",
    "                            threshold_step = filtered_max - five_percent_of_unfiltered_max\n",
    "                            first_index_above = df_sorted[df_sorted[\"global_step\"] >= threshold_step].index.min()\n",
    "                            B_min = first_index_above\n",
    "                            result_df = df_sorted.loc[B_min:]\n",
    "                            while (\n",
    "                                result_df[metric_key1].count() < 10\n",
    "                            ):  # Minimum number of points needed to consider the window.\n",
    "                                idx_max += 1\n",
    "                                threshold_step = df_steps_max[idx_max] - five_percent_of_unfiltered_max\n",
    "                                # Recalculate B_min based on the new threshold\n",
    "                                first_index_above = df_sorted[df_sorted[\"global_step\"] >= threshold_step].index.min()\n",
    "\n",
    "                                B_min = first_index_above\n",
    "                                B_max = df_sorted[\n",
    "                                    df_sorted[\"global_step\"] <= df_steps_max[idx_max]\n",
    "                                ].index.max()  # Redefine B_max to ensure it stays within the new filtered_max\n",
    "\n",
    "                                # Update result_df with the new range\n",
    "                                result_df = df_sorted.loc[B_min:B_max]\n",
    "\n",
    "                                # If filtered_max reaches the minimum global_step or no further reduction is possible, break the loop\n",
    "                                if filtered_max <= df_sorted[\"global_step\"].min():\n",
    "                                    break\n",
    "\n",
    "                            N = B_max - B_min + 1\n",
    "\n",
    "                            # Store additional info if needed\n",
    "                            ratio_steps.append((result_df[\"global_step\"].min(), result_df[\"global_step\"].max()))\n",
    "\n",
    "                            ratio_info.append(\n",
    "                                (\n",
    "                                    df_sorted[\"config/optim/num_epochs\"].values[0],\n",
    "                                    df_sorted[\"config/working_dir\"].values[0],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                            # Prepare and calculate combined metric\n",
    "                            f_1 = np.clip(result_df[metric_key1], a_min=-np.inf, a_max=10e12)\n",
    "                            f_2 = np.clip(result_df[metric_key2], a_max=np.inf, a_min=10e-12)\n",
    "                            ratio_nb.append((result_df[metric_key1].count(), result_df[metric_key2].count()))\n",
    "                            ratio_N_nb.append(N)\n",
    "                            combined_metric = (f_1 / f_2).mean()\n",
    "                            ratio_res.append(combined_metric)\n",
    "                            aggregated_data[keyword_][metric].append(combined_metric)\n",
    "                        else:\n",
    "                            if metric == \"batch/perf/avg_return_raw\":\n",
    "                                threshold_step = filtered_df[\"global_step\"].max() * 0.95  # Calculate the 95% threshold\n",
    "\n",
    "                                df_sorted = filtered_df.sort_values(by=\"global_step\", ascending=True)\n",
    "\n",
    "                                first_index_above = df_sorted[\n",
    "                                    df_sorted[\"global_step\"] >= threshold_step\n",
    "                                ].index.min()  # Find the first index where 'global_step' exceeds the threshold\n",
    "                                B_min = first_index_above\n",
    "                                B_max = df_sorted.index.max()\n",
    "                                N = B_max - B_min + 1\n",
    "\n",
    "                                result_df = df_sorted.loc[B_min:]  # Select this and all subsequent batches\n",
    "                                return_steps.append((result_df[\"global_step\"].min(), result_df[\"global_step\"].max()))\n",
    "                                final_rows = []\n",
    "                                removed_count = 0\n",
    "                                last_timestep = float(\"inf\")\n",
    "                                return_info.append(\n",
    "                                    (\n",
    "                                        df_sorted[\"config/optim/num_epochs\"].values[0],\n",
    "                                        df_sorted[\"config/working_dir\"].values[0],\n",
    "                                    )\n",
    "                                )\n",
    "                                for _, row in result_df.iterrows():\n",
    "                                    if not final_rows:\n",
    "                                        final_rows.append(row)  # Add the first row automatically\n",
    "                                        last_timestep = row[\"batch/perf/max_timestep\"]\n",
    "                                    else:\n",
    "                                        last_row = final_rows[-1]  # Check the last point added\n",
    "                                        if (\n",
    "                                            row[\"batch/perf/avg_return_raw\"] == last_row[\"batch/perf/avg_return_raw\"]\n",
    "                                        ):  # if duplicate\n",
    "                                            if (\n",
    "                                                row[\"batch/perf/max_timestep\"] <= last_timestep or removed_count >= 8\n",
    "                                            ):  # if we have already removed 8 points (duplicates) or if the max time step is lower than the previous one\n",
    "                                                removed_count = 0  # Reset the count\n",
    "                                                final_rows.append(row)  # Add this row as a valid point after reset\n",
    "                                            else:\n",
    "                                                removed_count += 1  # Else Increment removed count for duplicates, we are still in the episode\n",
    "                                        else:\n",
    "                                            # Add to final rows if it's not a duplicate\n",
    "                                            final_rows.append(row)\n",
    "                                            removed_count = 0  # Reset the count since a valid point was added\n",
    "                                        last_timestep = row[\"batch/perf/max_timestep\"]\n",
    "                                return_nb.append(len(final_rows))\n",
    "                                N_nb.append(N)\n",
    "\n",
    "                                final_df = pd.DataFrame(final_rows).sort_values(by=\"global_step\", ascending=False)\n",
    "\n",
    "                                mean_value = final_df[\n",
    "                                    \"batch/perf/avg_return_raw\"\n",
    "                                ].mean()  # Calculate the mean of the metric\n",
    "                                return_res.append(mean_value)\n",
    "                                # Storing the result\n",
    "                                aggregated_data[keyword_][metric].append(mean_value)\n",
    "\n",
    "                            else:\n",
    "                                threshold_step = filtered_df[\"global_step\"].max() * 0.95  # Calculate the 95% threshold\n",
    "\n",
    "                                df_sorted = filtered_df.sort_values(by=\"global_step\", ascending=True)\n",
    "\n",
    "                                first_index_above = df_sorted[\n",
    "                                    df_sorted[\"global_step\"] >= threshold_step\n",
    "                                ].index.min()  # Find the first index where 'global_step' exceeds the threshold\n",
    "                                B_min = first_index_above\n",
    "                                B_max = df_sorted.index.max()\n",
    "                                N = B_max - B_min + 1\n",
    "\n",
    "                                result_df = df_sorted.loc[B_min:]  # Select this and all subsequent batches\n",
    "\n",
    "                                mean_value = result_df[metric].mean()  # Calculate the mean of the metric\n",
    "\n",
    "                                aggregated_data[keyword_][metric].append(mean_value)  # Storing the result\n",
    "\n",
    "    config_keys2 = [key for key in df2.columns if key.startswith(\"config/\")]\n",
    "    unique_configs2 = df2[config_keys2].drop_duplicates()\n",
    "    i = 0\n",
    "\n",
    "    for _, config_row2 in unique_configs2.iterrows():\n",
    "        mask2 = create_mask_for_config(df2, config_row2, config_keys2)\n",
    "        filtered_df2 = df2[mask2].copy()\n",
    "        for keyword in keywords:\n",
    "            if \"algo\" in config_row2[\"config/solve_dir\"]:\n",
    "                continue\n",
    "            else:\n",
    "                if keyword in config_row2[\"config/solve_dir\"]:\n",
    "                    if config_row2[\"config/loss/policy/kwargs/feature_trust_all_layers\"] == True:\n",
    "                        keyword_ = \"regularize-all-layers\"\n",
    "                    else:\n",
    "                        keyword_ = keyword\n",
    "                    metric = \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\"\n",
    "                    threshold_step = (\n",
    "                        filtered_df2[\"capacity-counters/env_steps\"].max() * 0.95\n",
    "                    )  # Calculate the 95% threshold\n",
    "\n",
    "                    df_sorted = filtered_df2.sort_values(by=\"capacity-counters/env_steps\", ascending=True)\n",
    "\n",
    "                    first_index_above = df_sorted[\n",
    "                        df_sorted[\"capacity-counters/env_steps\"] >= threshold_step\n",
    "                    ].index.min()  # Find the first index where 'global_step' exceeds the threshold\n",
    "                    B_min = first_index_above\n",
    "                    B_max = df_sorted.index.max()\n",
    "                    N = B_max - B_min + 1\n",
    "\n",
    "                    result_df = df_sorted.loc[B_min:]  # Select this and all subsequent batches\n",
    "\n",
    "                    mean_value = result_df[metric].mean()  # Calculate the mean of the metric\n",
    "\n",
    "                    aggregated_data[keyword_][metric].append(mean_value)  # Storing the result\n",
    "\n",
    "    meanlineprops = dict(linestyle=\"-.\", linewidth=3, color=\"red\")\n",
    "    fig, axes = plt.subplots(1, len(metrics) + 1, figsize=(20, 2))\n",
    "    medianprops = dict(linestyle=\"-.\", linewidth=3, color=\"black\")\n",
    "    metrics2 = metrics + [metric2]\n",
    "    # Uncomment if you want information about each point in the boxplot (return and prob ratio)\n",
    "\n",
    "    #     print(save_name.split(\"-\")[2])\n",
    "    #     print(f\"- Return : \\n\")\n",
    "    #     for i in range(len(N_nb)):\n",
    "    #         print(f\"{return_steps[i][0]} -> {return_steps[i][1]}, {return_nb[i]}/{N_nb[i]}, {return_info[i][0]},{return_info[i][1]},{return_res[i]}\")\n",
    "    #     print(f\"- Ratio : \\n\")\n",
    "    #     for i in range(len(ratio_N_nb)):\n",
    "    #         print(f\"{ratio_steps[i][0]} -> {ratio_steps[i][1]}, {ratio_nb[i]}/{ratio_N_nb[i]}, {ratio_info[i][0]},{ratio_info[i][1]},{ratio_res[i]}\")\n",
    "\n",
    "    for col_idx, metric in enumerate(metrics2):\n",
    "        m = []\n",
    "        ax = axes[col_idx]\n",
    "        for row_idx, keyword in enumerate(keywords):\n",
    "            if metric == \"batch/end/feature_stats/norm_features_preactivation_policy_batch\" or metric == \"ProbRatio\":\n",
    "                data = aggregated_data[keyword][metric]\n",
    "            else:\n",
    "                data = aggregated_data[keyword][metric]\n",
    "            m.append(data)\n",
    "            # print(f\"Metric: {metric}, Keyword: {keyword}, Data Points: {len(data)}\")\n",
    "\n",
    "        if metric in [\n",
    "            \"ProbRatio\",\n",
    "            \"batch/end/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "            \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "        ]:\n",
    "            bp = ax.boxplot(\n",
    "                m,\n",
    "                widths=0.6,\n",
    "                patch_artist=True,\n",
    "                vert=False,\n",
    "                medianprops=medianprops,\n",
    "                meanprops=meanlineprops,\n",
    "                showmeans=True,\n",
    "                meanline=True,\n",
    "                flierprops=dict(\n",
    "                    marker=\"o\", markerfacecolor=\"none\", markeredgecolor=\"black\", markersize=8, markeredgewidth=2\n",
    "                ),\n",
    "            )\n",
    "            ax.set_xscale(\"log\")\n",
    "        else:\n",
    "            bp = ax.boxplot(\n",
    "                m,\n",
    "                widths=0.6,\n",
    "                patch_artist=True,\n",
    "                vert=False,\n",
    "                medianprops=medianprops,\n",
    "                meanprops=meanlineprops,\n",
    "                showmeans=True,\n",
    "                meanline=True,\n",
    "                flierprops=dict(\n",
    "                    marker=\"o\", markerfacecolor=\"none\", markeredgecolor=\"black\", markersize=8, markeredgewidth=2\n",
    "                ),\n",
    "            )\n",
    "        # Set colors for each box\n",
    "        for patch, color in zip(bp[\"boxes\"], colors[: len(m)]):\n",
    "            patch.set_facecolor(color)\n",
    "        ax.set_title(f\"{metric_name[col_idx]}\", fontsize=13.5)\n",
    "        if col_idx == 0:\n",
    "            ax.set_yticks([y + 1 for y in range(len(m))], labels=key)\n",
    "            # ax.set_xlabel(f\"\\n{config_row['config/env/name']}\")\n",
    "        if col_idx > 0 and isinstance(ax, plt.Axes):\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_yticks([])\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    mean_line = lines.Line2D([], [], color=\"red\", linestyle=\"-.\", linewidth=3, label=\"Mean\")\n",
    "    median_line = lines.Line2D([], [], color=\"black\", linestyle=\"-.\", linewidth=3, label=\"Median\")\n",
    "    outlier_marker = lines.Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        label=\"Outliers\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=\"black\",\n",
    "        markersize=8,\n",
    "        markeredgewidth=2,\n",
    "    )\n",
    "\n",
    "    axes[-1].legend(\n",
    "        handles=[mean_line, median_line, outlier_marker],\n",
    "        loc=\"upper center\",  # This anchors the center of the legend at the provided coordinate\n",
    "        bbox_to_anchor=(-2.8, 1.4),  # Anchors the legend above the plot\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=12,\n",
    "        ncol=3,\n",
    "        frameon=False,\n",
    "        handlelength=1,\n",
    "        handletextpad=0.5,\n",
    "        columnspacing=1,\n",
    "    )\n",
    "    fig.text(-0.015, 0.01, f'{save_name.split(\"-\")[1]}', ha=\"left\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.savefig(f\"../outputs/rlconf-plotting/plots/boxplot-{save_name}.pdf\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3503ed3",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ca8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure6_mujoco(name, activation):\n",
    "    criteria = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "    }\n",
    "    df_check = filter_df_by_criteria(mujoco_df, criteria)\n",
    "    criteria2 = {\n",
    "        (\"config\", \"env\", \"name\"): [\n",
    "            name,\n",
    "        ],\n",
    "        (\"config\", \"models\", \"activation\"): activation,\n",
    "        (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "    }\n",
    "    f_name = f\"6-{name}-{activation}\"\n",
    "    f_name = f_name.replace(\"/\", \"-\")\n",
    "    df_check2 = filter_df_by_criteria(mujoco_df_capacity, criteria2)\n",
    "    config_keywords = [\"shared-trunk\", \"optimizer\", \"regularize-all-layers\", \"regularize\", \"baselines\"]\n",
    "    metrics = [\n",
    "        \"batch/perf/avg_return_raw\",\n",
    "        \"ProbRatio\",\n",
    "        \"batch/end/dead_neurons/features_policy_batch\",\n",
    "        \"batch/end/feature_stats/norm_features_preactivation_policy_batch\",\n",
    "        \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "    ]\n",
    "    metric_name = [\n",
    "        \"Episode return\",\n",
    "        \"Excess ratio\",\n",
    "        \"Dead neurons policy\",\n",
    "        \"Norm preactivation policy\",\n",
    "        \"Feature rank policy (PCA)\",\n",
    "        \"Plasticity loss policy\",\n",
    "    ]\n",
    "    plot_metrics_by_keywords_mujoco(\n",
    "        df_check,\n",
    "        df_check2,\n",
    "        metrics,\n",
    "        \"capacity-checkpoint/last_capacity-epoch/last_capacity-minibatch/loss/policy\",\n",
    "        metric_name,\n",
    "        config_keywords,\n",
    "        save_name=f_name,\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11308a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure6_mujoco(\"Humanoid-v4\", \"Tanh\")\n",
    "figure6_mujoco(\"Hopper-v4\", \"Tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64551a",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure6_mujoco(\"Humanoid-v4\", \"ReLU\")\n",
    "figure6_mujoco(\"Hopper-v4\", \"ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ad66b",
   "metadata": {},
   "source": [
    "## Correlation between the 5 ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0fc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_keys2 = [\n",
    "    \"batch/end/SVD/effective_rank_vetterli/features_policy_batch\",\n",
    "    \"batch/end/SVD/approximate_rank_pca/features_policy_batch\",\n",
    "    \"batch/end/SVD/srank_kumar/features_policy_batch\",\n",
    "    \"batch/end/SVD/feature_rank_lyle/features_policy_batch\",\n",
    "    \"batch/end/SVD/pytorch_rank/features_policy_batch\",\n",
    "]\n",
    "# Example usage with multiple options for a criterion:\n",
    "criteria_tanh = {\n",
    "    (\"config\", \"working_dir\"): \"baselines\",\n",
    "    (\"config\", \"models\", \"activation\"): \"Tanh\",\n",
    "    (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "}\n",
    "\n",
    "\n",
    "criteria_relu = {\n",
    "    (\"config\", \"working_dir\"): \"baselines\",\n",
    "    (\"config\", \"models\", \"activation\"): \"ReLU\",\n",
    "    (\"config\", \"optim\", \"anneal_linearly\"): [False],\n",
    "}\n",
    "\n",
    "filtered_df_tanh = filter_df_by_criteria(mujoco_df, criteria_tanh)\n",
    "filtered_df_relu = filter_df_by_criteria(mujoco_df, criteria_relu)\n",
    "\n",
    "correlations_tanh = average_correlations_ranks(filtered_df_tanh, metrics_keys2)\n",
    "correlations_relu = average_correlations_ranks(filtered_df_relu, metrics_keys2)\n",
    "plot_correlation_matrices(correlations_tanh, metrics_keys2, \"avg_mujoco_tanh\")\n",
    "plot_correlation_matrices(correlations_relu, metrics_keys2, \"avg_mujoco_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71452873",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_correlations_score_tanh = worst_correlations(filtered_df_tanh, metrics_keys2)\n",
    "worst_correlations_score_relu = worst_correlations(filtered_df_relu, metrics_keys2)\n",
    "plot_correlation_matrices(worst_correlations_score_tanh, metrics_keys2, \"worst_mujoco_tanh\")\n",
    "plot_correlation_matrices(worst_correlations_score_relu, metrics_keys2, \"worst_mujoco_relu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
